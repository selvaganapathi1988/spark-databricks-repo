{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ba86a20-5a3a-4130-86f5-e312f4a7901b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Telecom Domain ReadOps <br/>Assignment\n",
    "This notebook contains assignments to practice Spark read options and Databricks volumes. <br>\n",
    "Sections: Sample data creation, Catalog & Volume creation, Copying data into Volumes, Path glob/recursive reads, toDF() column renaming variants, inferSchema/header/separator experiments, and exercises.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "841c7ed8-ef18-486a-8187-07685e499b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://fplogoimages.withfloats.com/actual/68009c3a43430aff8a30419d.png)\n",
    "![](https://theciotimes.com/wp-content/uploads/2021/03/TELECOM1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4aa0a44-8cd6-41cf-921d-abb5ff67615b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##First Import all required libraries & Create spark session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3415d064-33c3-4af6-b70f-fa05ef06a3d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b67823-2e4e-45e2-aa25-80550a3ac580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Write SQL statements to create:\n",
    "1. A catalog named telecom_catalog_assign\n",
    "2. A schema landing_zone\n",
    "3. A volume landing_vol\n",
    "4. Using dbutils.fs.mkdirs, create folders:<br>\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\n",
    "5. Explain the difference between (Just google and understand why we are going for volume concept for prod ready systems):<br>\n",
    "a. Volume vs DBFS/FileStore<br>\n",
    "b. Why production teams prefer Volumes for regulated data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da3a87d-0732-430d-b3bc-b137ee14b51f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists telecom_catalog_assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f65ca5f-4dba-4a67-8ec6-f3282949c528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create schema if not exists telecom_catalog_assign.landing_zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "986b8480-7099-4309-9fed-f758602a7deb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create volume if not exists telecom_catalog_assign.landing_zone.landing_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f9885f-72d9-476e-b7c7-8b073ce6d6c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer\"\n",
    "    )\n",
    "dbutils.fs.mkdirs(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage\"\n",
    "    )\n",
    "dbutils.fs.mkdirs(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower\"\n",
    "    )\n",
    "\n",
    "dbutils.fs.mkdirs(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1\"\n",
    "    )\n",
    "\n",
    "dbutils.fs.mkdirs(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ad3ae6f-4302-4a66-a122-2030e1865c7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "a. Volume vs DBFS/FileStore<br/>\n",
    "\n",
    "Volume:<br/><br/>\n",
    "Production teams choose Volumes because they provide the necessary tools for data security, compliance, and operational excellence for sensitive, regulated data (e.g., HIPAA, GDPR, CCPA). <br/>\n",
    "  1.Centralized Governance:<br/>\n",
    "  Fully governed by Unity Catalog.Volumes integrate seamlessly with Databricks Unity Catalog, which acts as a single, unified governance solution across all workspaces. This eliminates fragmented access controls and inconsistent security policies that were common with older methods like DBFS mounts.\n",
    "\n",
    "  2.Data Types: Governs non-tabular data (images, CSV, JSON, libraries, ML models, etc.) in cloud storage.\n",
    "\n",
    "  3.Path Format: Uses a three-level namespace path: /Volumes/catalog/schema/volume/path.\n",
    "\n",
    "  4.Databricks Recommendation:  Recommended for storing all non-tabular data.\n",
    "\n",
    "  DBFS/Filestore:<br/>\n",
    "\n",
    "  Relies on legacy workspace-level ACLs or cloud IAM roles; complex to manage.<br/>\n",
    "\n",
    "  Permissions are less granular access<br/>\n",
    "\n",
    "  Deprecated for most uses; not recommended for important data due to security concerns.\n",
    "\n",
    "  b. Why production teams prefer Volumes for regulated data?<br/>\n",
    "  Production teams favor Volumes due to the robust governance framework provided by Databricks Unity Catalog, which is critical for handling sensitive and regulated data (e.g., data subject to GDPR, HIPAA, CCPA). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26d8bd3d-b575-448b-ae22-8173d15ca671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data files to use in this usecase:\n",
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d25c1957-f9b2-4072-82df-438fbd02ee61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9540d2e2-2562-4be7-897f-0a7d57adaa72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Filesystem operations\n",
    "1. Write code to copy the above datasets into your created Volume folders:\n",
    "Customer → /Volumes/.../customer/\n",
    "Usage → /Volumes/.../usage/\n",
    "Tower (region-based) → /Volumes/.../tower/region1/ and /Volumes/.../tower/region2/\n",
    "\n",
    "2. Write a command to validate whether files were successfully copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f9f659-de40-4a4c-b06f-4b825d1ffe19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_csv.csv\", customer_csv,overwrite=True)\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_tsv.csv\", usage_tsv,overwrite=True)\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\", tower_logs_region1,overwrite=True)\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/tower_logs_region2.csv\", tower_logs_region1,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe67b29-7139-4c05-b86d-ea55adb2f52a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh ls -ltr /Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/ /Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage /Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1 /Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8767735b-24d3-428a-ad12-ae821903e2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Directory Read Use Cases\n",
    "1. Read all tower logs using:\n",
    "Path glob filter (example: *.csv)\n",
    "Multiple paths input\n",
    "Recursive lookup\n",
    "\n",
    "2. Demonstrate these 3 reads separately:\n",
    "Using pathGlobFilter\n",
    "Using list of paths in spark.read.csv([path1, path2])\n",
    "Using .option(\"recursiveFileLookup\",\"true\")\n",
    "\n",
    "3. Compare the outputs and understand when each should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fbd16ba-22fb-4401-bf50-b4ad0dd9a9f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tower_df1=spark.read.option(\"header\",\"True\").option(\"recursiveFileLookup\",\"True\").option(\"pathGlobFilter\",\"*.csv\").option(\"inferschema\",\"True\").option(\"sep\",\"|\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower\")\n",
    "display(tower_df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c99c3b89-4d81-4ab3-a428-6381c7035911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df1=spark.read.options(pathGlobFilter=\"*.csv\",inferSchema=\"True\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer\")\n",
    "display(customer_df1)\n",
    "\n",
    "tower_df1=spark.read.option(\"sep\",\"|\").csv([\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/\",\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/\"])\n",
    "display(tower_df1)\n",
    "\n",
    "recurs_df1=spark.read.option(\"recursiveFileLookup\",\"True\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/\")\n",
    "display(recurs_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f7147c1-5d58-47e1-84fe-7ebd26a217b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Schema Inference, Header, and Separator\n",
    "1. Try the Customer, Usage files with the option and options using read.csv and format function:<br>\n",
    "header=false, inferSchema=false<br>\n",
    "or<br>\n",
    "header=true, inferSchema=true<br>\n",
    "2. Write a note on What changed when we use header or inferSchema  with true/false?<br>\n",
    "3. How schema inference handled “abc” in age?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4336f2ff-20c6-4980-ae37-e728ee37f081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df1=spark.read.option(\"header\",\"False\").option(\"inferSchema\",\"False\").format(\"csv\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_csv.csv\")\n",
    "display(customer_df1)\n",
    "customer_df1.printSchema()\n",
    "\n",
    "\n",
    "usage_df1=spark.read.option(\"header\",\"False\").option(\"inferSchema\",\"False\").option(\"sep\",\"\\t\").format(\"csv\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_tsv.csv\")\n",
    "display(usage_df1)\n",
    "usage_df1.printSchema()\n",
    "\n",
    "\n",
    "customer_df2=spark.read.options(header=\"True\",inferSchema=\"True\").format(\"csv\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_csv.csv\")\n",
    "display(customer_df2)\n",
    "customer_df2.printSchema()\n",
    "\n",
    "usage_df2=spark.read.options(header=\"True\",inferSchema=\"True\",sep=\"\\t\").format(\"csv\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_tsv.csv\")\n",
    "display(usage_df2)\n",
    "usage_df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fe709fb-8c75-4bf6-a65b-962249d83a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Write a note on What changed when we use header<br/> or inferSchema with true/false?\n",
    "Header(False):It will not consider first row value as column/header.default colmun assignment is c0,c1,c2..<br/>\n",
    "inferSchema(False):It will apply the deafult variable type for the column as string.\n",
    "\n",
    "Header(True): It will consider the first row as column/header name.<br/>\n",
    "inferSchema(True):It will apply values passed in input value accordingly for variable type for the column. If any string value passed with other datatypes means, it will take the type as string\n",
    "\n",
    "#######How schema inference handled “abc” in age?\n",
    "Here age should be always integertype but here \" abc\" is there in age so it is infering as string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "874f8a75-f894-498a-b05d-26bfa7337724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15d8dad0-bc63-47f1-9a90-72837cba6c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Column Renaming Usecases\n",
    "1. Apply column names using string using toDF function for customer data\n",
    "2. Apply column names and datatype using the schema function for usage data\n",
    "3. Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef04ba24-247b-4306-9806-e474201f7123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df1=spark.read.options(header=\"False\",inferSchema=\"False\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_csv.csv\").toDF(\"Id\",\"Name\",\"Age\",\"City\",\"Plan\")\n",
    "display(customer_df1)\n",
    "customer_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e9d2f2d-2cfa-4ca1-b33c-c1df693249bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "schema_define=StructType([StructField(\"Customer_id\",IntegerType(),True),StructField(\"Voice_mins\",IntegerType(),True),StructField(\"Data_mb\",IntegerType(),True),StructField(\"SMS_count\",IntegerType(),True)])\n",
    "usage_df1=spark.read.options(header=\"True\",inferSchema=\"True\",sep=\"\\t\").schema(schema_define).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_tsv.csv\")\n",
    "display(usage_df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d89660a-161b-4f5b-8324-68fc99b549f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_df1=spark.read.options(header=\"True\",inferSchema=\"True\",sep=\"\\t\").schema(\"Customer_id int,Voice_mins int,Data_mb int,SMS_count int\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_tsv.csv\")\n",
    "display(usage_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e18c0e-bf4e-436e-8343-7999781c7082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType,TimestampType\n",
    "schema_define=StructType([StructField(\"event_id\",IntegerType(),True),StructField(\"Customer_id\",IntegerType(),True),StructField(\"tower_id\",StringType(),True),StructField(\"signal_strength\",IntegerType(),True),StructField(\"timestamp\",TimestampType(),True)])\n",
    "usage_df1=spark.read.options(header=\"True\",inferSchema=\"True\",sep=\"|\").schema(schema_define).csv(path=[\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\",\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2/tower_logs_region2.csv\"])\n",
    "display(usage_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e1d6d88-7bcc-4548-a0d1-15d37f6fc0be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. More to come (stay motivated)...."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8306938983722378,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4_Usecases_read_write_workout",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
