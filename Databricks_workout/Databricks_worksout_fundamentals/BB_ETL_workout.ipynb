{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae915761-8706-4042-bc87-7fba66c59287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec0b320-5927-49c8-b52c-9dcc11ea6083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_df1=spark.read.csv(\"/Volumes/workspace/default/volume1/custsmodified\")\n",
    "display(read_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1302f4ca-b79f-4951-acb6-3a8e576a700b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read_df1.write.csv(\"/Volumes/workspace/default/volumewd36/ingestion_volume/source/custsmodified\",mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cc61dd2-1802-41e0-8f4b-e01e51426351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####a. Passive Data Munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9314e13b-d9aa-4502-8da4-11a6d2110882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv(\"/Volumes/workspace/default/volumewd36/ingestion_volume/source/custsmodified\",header=False,inferSchema=True).toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\")\n",
    "r_c=rawdf1.count()\n",
    "print(\"Total row count\",r_c)\n",
    "rawdf1.show(20,False)\n",
    "display(rawdf1.take(21))\n",
    "display(rawdf1.sample(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a815425b-3a7f-448c-a918-393863b82043",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78597c99-934c-4efb-9859-d04f05b38646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(rawdf1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e13faa26-61d4-40d8-a111-a148893632e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(rawdf1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3aed4c7-fff2-440c-8c95-2e88056d03dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in rawdf1.dtypes:\n",
    "    if i[1]=='string':\n",
    "        print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "699760f5-ec81-4e29-9da0-ecf920424b8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(rawdf1.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "862f27fb-7472-4d67-b8f1-d0f5a7d69893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Actual total record\",rawdf1.count())\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.distinct().count())\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.dropDuplicates().count())\n",
    "print(\"de-duplicated given cid column count\",rawdf1.dropDuplicates(['id']).count())\n",
    "display(rawdf1.describe())\n",
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdfd0f85-ae79-4d17-9500-c73d433d8e3f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "####b. Active Data Munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "001b0ac6-3bf0-408b-ae0c-4a0c57a7ab06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession  #approx 15lakh bulidin funct\n",
    "spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9f4742e-5968-4f6a-a0c1-f6d224a50c90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extraction(Ingestion) method\n",
    "#single file path\n",
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(\"/Volumes/workspace/default/volumewd36/ingestion_volume/source/custsmodified\")\n",
    "\n",
    "#multiple path\n",
    "struct2=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf2=spark.read.schema(struct2).csv(path=[\"/Volumes/workspace/default/volumewd36/ingestion_volume/source/custsmodified\",\"/Volumes/workspace/default/volumewd36/ingestion_volume/source/custsmodified\"])\n",
    "\n",
    "#multiple files in multiple path\n",
    "struct2=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf3=spark.read.schema(struct2).csv(path=[\"/Volumes/workspace/default/volumewd36/ingestion_volume/source/custsmodified\",\"/Volumes/workspace/default/volumewd36/ingestion_volume/source/source2/\"],pathGlobFilter=\"cust*\",recursiveFileLookup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2329fcc9-1680-4972-a001-8b342158a499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Active data munging..\n",
    "\n",
    "struct5=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf5=spark.read.schema(struct5).csv(\"/Volumes/workspace/default/volumewd36/ingestion_volume/source/\",pathGlobFilter=\"custsmodified_N*\",recursiveFileLookup=True)\n",
    "\n",
    "struct6=\"id string, firstname string, age string, profession string,city string\"\n",
    "rawdf6=spark.read.schema(struct6).csv(\"/Volumes/workspace/default/volumewd36/ingestion_volume/source/\",pathGlobFilter=\"custsmodified_T*\",recursiveFileLookup=True)\n",
    "\n",
    "display(rawdf5)\n",
    "display(rawdf6)\n",
    "\n",
    "rawdf7=rawdf5.union(rawdf6)    #this method not correct, see the output\n",
    "display(rawdf7)\n",
    "\n",
    "rawdf8=rawdf5.unionByName(rawdf6,allowMissingColumns=True)  #this is correct method, if we have 2 diff files and diff coulmn\n",
    "display(rawdf8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4b3f6fa-7016-42cb-9b9c-07f895d63762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Validation by doing cleansing\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ShortType,IntegerType\n",
    "\n",
    "struttype1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True)])\n",
    "\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/workspace/default/volumewd36/ingestion_volume/source/custsmodified\",mode='permissive')\n",
    "print(\"after keeping nulls on the wrong data format\",cleandf1.count())#all rows count\n",
    "display(cleandf1)#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato)\n",
    "#or\n",
    "#method2 - drop malformed rows\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/workspace/default/volumewd36/ingestion_volume/source/custsmodified\",mode='dropMalformed')\n",
    "print(\"after cleaning wrong data (type mismatch, column number mismatch)\",len(cleandf1.collect()))\n",
    "display(cleandf1)#We are removing the entire row, where ever data format mismatch is there (throwing away the entire potato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8f4f42e-b5e9-43bb-bec5-c2582f2198a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#method3 best methodology of applying active data munging\n",
    "#Validation by doing cleansing (not at the time of creating Dataframe, rather we will clean and scrub subsequently)...\n",
    "struttype1=StructType([StructField('id', StringType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', StringType(), True), StructField('profession', StringType(), True)])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "rawdf9=spark.read.schema(struttype1).csv(path=\"/Volumes/workspace/default/volumewd36/ingestion_volume/source/custsmodified\",mode='permissive')\n",
    "print(\"allow all data showing the real values\",rawdf1.count())#all rows count\n",
    "display(rawdf9)#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65dda59a-79a4-49e7-9f67-0815745df416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Rejection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f7bf44b-4209-4f50-bb4c-baf6438db5ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creating rejection dataset to send to our source system for future fix\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ShortType,IntegerType\n",
    "struttype1=StructType([StructField('id', IntegerType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', ShortType(), True), StructField('profession', StringType(), True),StructField(\"corruptedrows\",StringType())])\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "cleandf1=spark.read.schema(struttype1).csv(path=\"/Volumes/workspace/default/volumewd36/ingestion_volume/source/custsmodified\",mode='permissive',columnNameOfCorruptRecord=\"corruptedrows\")\n",
    "#Create a reject dataset\n",
    "rejectdf1=cleandf1.where(\"corruptedrows is not null\")\n",
    "display(rejectdf1)\n",
    "rejectdf1.write.csv(\"/Volumes/workspace/default/volumewd36/ingestion_volume/source/source/reject\",mode=\"overwrite\",header=True)\n",
    "retaineddf1=cleandf1.where(\"corruptedrows is null\")\n",
    "print(\"Overall rows in the source data is \",len(cleandf1.collect()))\n",
    "print(\"Rejected rows in the source data is \",len(rejectdf1.collect()))\n",
    "print(\"Clean rows in the source data is \",len(retaineddf1.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e5df76e-0840-475f-a132-eb183abe4c77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e940dd21-c9e9-42c4-9097-fb4b53bb1f36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####Cleansing\n",
    "\n",
    "#We already know how to do cleansing applying the strict Structure on method1 and method2\n",
    "#Important na functions we can use to do cleansing\n",
    "cleanseddf1=rawdf9.na.drop(how=\"any\")#This function will drop any column in a given row with null otherwise this function returns rows with no null columns\n",
    "display(cleanseddf1.where(\"age is null\"))\n",
    "display(rawdf9.where(\"age is null\"))\n",
    "cleanseddf1=rawdf9.na.drop(how=\"any\",subset=[\"id\",\"age\"])\n",
    "display(cleanseddf1)\n",
    "cleanseddf1=rawdf9.na.drop(how=\"all\",subset=[\"lastname\",\"profession\"])\n",
    "display(cleanseddf1)\n",
    "print(\"any one row in the cleansed df with firstname and lastname is null\")\n",
    "print(\"Total rows without firstname and lastname with null values\",len(cleanseddf1.collect()))\n",
    "display(cleanseddf1)#We are taking this DF further for munging.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e8f7081-72cd-4938-9347-4712e659c4d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scrubbeddf1=cleanseddf1.na.fill('not provided',subset=[\"lastname\",\"profession\"])#fill will help us replace nulls with some value\n",
    "display(scrubbeddf1)\n",
    "find_replace_values_dict1={'Pilot':'Captain','Actor':'Celeberity'}\n",
    "find_replace_values_dict2={'not provided':'NA'}\n",
    "find_replace_values_dict3={'not provided':'NA'}\n",
    "scrubbeddf2=scrubbeddf1.na.replace(find_replace_values_dict1,subset=[\"profession\"])\n",
    "display(scrubbeddf2)\n",
    "scrubbeddf3=scrubbeddf2.na.replace(find_replace_values_dict2,subset=[\"lastname\"])\n",
    "display(scrubbeddf3)\n",
    "scruppeddf4=scrubbeddf2.na.replace(find_replace_values_dict3,subset=[\"lastname\"])\n",
    "display(scruppeddf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa49efb5-c794-47e4-9321-9ecd76aea4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c2eb629-ade1-45d6-9f2e-b059864be631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(scrubbeddf3.where(\"id in ('4000001')\"))#before row level dedup\n",
    "dedupdf1=scrubbeddf3.distinct()#It will remove the row level duplicates\n",
    "display(dedupdf1.where(\"id in ('4000001')\"))\n",
    "display(dedupdf1.coalesce(1).where(\"id in ('4000003')\"))\n",
    "\n",
    "dedupdf2=dedupdf1.coalesce(1).dropDuplicates(subset=[\"id\"])\n",
    "display(dedupdf2.where(\"id in ('4000003')\"))\n",
    "\n",
    "dedupdf3=dedupdf1.coalesce(1).orderBy([\"id\",\"age\"],ascending=[True,True]).dropDuplicates(subset=[\"id\"])\n",
    "display(dedupdf3.where(\"id in ('4000003')\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40f19470-b805-432c-aa4b-e0db406ca64e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Standaradization--Adding column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca76e62-396c-4577-aa75-dc1c4c49a1ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,initcap,col\n",
    "#withColumn(\"stringcolumnname to add in the df\",lit('hardcoded')/initcap(col(\"colname\")))\n",
    "standarddf1=dedupdf3.withColumn(\"sourcesystem\",lit(\"Retail\"))#SparkSQL - DSL(FBP)\n",
    "display(standarddf1.limit(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50a2d350-9570-4cb9-8d9d-a009bd758e63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Coulmn Uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66acb1b5-f114-4bc7-b20a-fba11be71c95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "\n",
    "#Standardization2 - column uniformity\n",
    "standarddf2=standarddf1.withColumn(\"profession\",initcap(\"profession\"))#inicap or any other string function with columnOr name can accept either column or string type provided if the string is a column name for eg. profession/age/sourcesystem.\n",
    "display(standarddf2.limit(20))\n",
    "display(standarddf2.groupBy(\"profession\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ecdff23-3988-4f58-8261-502c236b8b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Standardization3 - Format Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17465343-f67e-4003-82ab-6c6ca5e95342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Did analysis to understand the format issues in our id and age columns\n",
    "#standarddf2.where(\"id like 't%'\").show()\n",
    "\n",
    "standarddf2.where(\"id rlike '[a-zA-Z]'\").show()\n",
    "#standarddf2.where(\"id rlike '[a-zA-Z]'\").show()#rlike is regular expression like function that help us identify any string data in our DF column\n",
    "standarddf2.where(\"age rlike '[^0-9]'\").show()#checking for any non number values in age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7179c12-62e1-435e-b11e-c7004c0b315b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace,replace\n",
    "#Let's apply scrubbing features to our id column to replace ten with 10 (or we can think of using GenAI here)\n",
    "replaceval={'one':'1','two':'2','three':'3','four':'4','five':'5','six':'6','seven':'7','eight':'8','nine':'9','ten':'10'}\n",
    "standarddf3=standarddf2.na.replace(replaceval,[\"id\"])\n",
    "#standarddf3=standarddf2.withColumn(\"id\",replace(\"id\",lit('ten'),\"10\"))\n",
    "standarddf3=standarddf3.withColumn(\"age\",regexp_replace(\"age\",'-',\"\"))\n",
    "display(standarddf3)\n",
    "\n",
    "standarddf4=standarddf3.where(\"id='10'\")\n",
    "display(standarddf4)\n",
    "\n",
    "standarddf4=standarddf3.where(\"age='77'\")\n",
    "display(standarddf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36767047-563a-45ed-8643-f8cc19cdadee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Standardization4 - Data Type Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "261d2b23-9d09-49e4-9646-16f28aca4bef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf3.printSchema()#Still id and age are string type, though it contains int data\n",
    "standarddf4.withColumn(\"id\",standarddf3.id.cast(\"long\"))      #---------not working\n",
    "standarddf4.printSchema()\n",
    "\n",
    "standarddf4=standarddf3.withColumn(\"id\",standarddf3[\"id\"].cast(\"int\"))    #----->working\n",
    "standarddf4.printSchema()\n",
    "\n",
    "standarddf4=standarddf3.withColumn(\"id\",col(\"id\").cast(\"long\"))       #------>working\n",
    "standarddf4.printSchema()\n",
    "\n",
    "standarddf4=standarddf4.withColumn(\"age\",col(\"age\").cast(\"short\"))      #---->working\n",
    "standarddf4.printSchema()\n",
    "\n",
    "display(standarddf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68310182-3c89-4a60-ac05-2ec5c6dcdf7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Standardization5 - Naming Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f155a156-d651-44a1-8f3f-21aaada7ad2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf5=standarddf4.withColumnRenamed(\"id\",\"custid\")\n",
    "standarddf5=standarddf4.withColumnsRenamed({\"id\":\"custid\",\"sourcesystem\":\"srcsystem\"})\n",
    "display(standarddf5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e280b3b-5a14-48cf-8a97-e3841604f582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization6 - Reorder Standadization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2294f91d-8d52-46f4-9742-10a02bb6095e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reordering the column selecting.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a615ede6-0895-4f3e-9365-408b4a6250a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standarddf6=standarddf5.select(\"custid\", \"age\", \"firstname\",\"lastname\",\"profession\",\"srcsystem\")\n",
    "mungeddf=standarddf6\n",
    "display(mungeddf.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b22b0afd-0e8d-4c84-b8db-035a9170124d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**2. Data Enrichment** - Detailing of data\n",
    "Makes your data rich and detailed <br>\n",
    "a. Add (withColumn,select,selectExpr), Derive (withColumn,select,selectExpr), Remove(drop,select,selectExpr), Rename (withColumnRenamed,select,selectExpr), Modify/replace (withColumn,select,selectExpr) - very important spark sql functions <br>\n",
    "b. split, merge/Concat <br>\n",
    "c. Type Casting, reformat & Schema Migration <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d24a330-e31b-4f0b-a414-15baa829e01f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "a. Add (), Derive (), Rename (), Modify/replace (), Remove/Eliminate () - very important spark sql DF functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d03101-cb22-4412-a42e-bee8655f2086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Adding of columns\n",
    "Lets add datadt (date of the data orgniated from the source for eg. provided in the filename in a format of yy/dd/MM) and loaddt (date when we are loading the data into our system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4b757af-b8f3-432e-bf69-be17e8b9b2ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "derived_datadt='25/30/12'\n",
    "print(f\"hello '{derived_datadt}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27e3456c-a68e-4e73-a2c4-8142ee882e01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,current_date#already imported, not needed here\n",
    "original_filename='custsmodified_25/30/12.csv'#We are deriving this date from the filename provided by the source custsmodified_25/30/12.csv\n",
    "derived_datadt=original_filename.split('_')[1].split('.')[0]\n",
    "#derived_datadt='25/30/12'#We are deriving this date from the filename provided by the source custsmodified_25/30/12.csv\n",
    "enrichdf1=mungeddf.withColumn(\"datadt\",lit('25/30/12')).withColumn(\"loaddt\",current_date())\n",
    "display(enrichdf1)\n",
    "enrichdf1.printSchema()\n",
    "\n",
    "enrichdf1=mungeddf.withColumns({\"datadt\":lit('25/30/12'),\"loaddt\":current_date()})\n",
    "display(enrichdf1)\n",
    "enrichdf1.printSchema()\n",
    "\n",
    "enricheddf1=mungeddf.select(\"*\",lit(derived_datadt).alias('datadt'),current_date().alias('loaddt'))\n",
    "display(enricheddf1)\n",
    "enricheddf1.printSchema()\n",
    "\n",
    "\n",
    "enrichdf1=mungeddf.selectExpr(\"*\",f\"'{derived_datadt}' as datadt\",\"current_date() as loaddt\")#DSL(select) + SQL expression\n",
    "enrichdf1.printSchema()\n",
    "display(enrichdf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a05ed767-768d-4fff-bd51-67d5ee79db8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Deriving of columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72b719be-2483-4f8e-b0ef-fc81ae078985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "bringing mentioned letters from column value, here substring/substr just bringing first letter by giving substr(colmnname,1,1)...index 1 and character lenghth 1 so first letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c36ac0-aaac-4188-8894-36b861e61247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "enrichdf2=enrichdf1.withColumn(\"professionflag\",substring(\"profession\",1,1))\n",
    "#or\n",
    "enrichdf2=enrichdf1.select(\"*\",substring(\"profession\",1,1).alias(\"professionflag\"))\n",
    "#or\n",
    "enrichdf2=enrichdf1.selectExpr(\"*\",\"substr(profession,1,1) as professionflag\")\n",
    "display(enrichdf2.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a8f59e-fe79-4e0e-8299-373ce5d929d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Renaming of Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faeb2a26-4d2f-4269-ab52-50e5cb4be5da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Can we use withColumn to rename? not directly, its costly\n",
    "enrichdf3=enrichdf2.withColumn(\"sourcename\",col(\"srcsystem\"))\n",
    "enrichdf3=enrichdf3.drop(\"srcsystem\").select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"sourcename\",\"datadt\",\"loaddt\",\"professionflag\")\n",
    "#or\n",
    "enrichdf3=enrichdf2.select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",col(\"srcsystem\").alias(\"sourcename\"),\"datadt\",\"loaddt\",\"professionflag\")#costly too, since we have to choose all columns in the select\n",
    "#or\n",
    "#enrichdf2.printSchema()\n",
    "enrichdf3=enrichdf2.selectExpr(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"srcsystem as sourcename\",\"datadt\",\"loaddt\",\"professionflag\")#costly too, since we have to choose all columns in the select\n",
    "#or\n",
    "enrichdf3=enrichdf2.withColumnRenamed(\"srcsystem\",\"sourcename\")#Best function to rename the column(s)\n",
    "#or\n",
    "enrichdf3=enrichdf2.withColumnsRenamed({\"srcsystem\":\"sourcename\",\"professionflag\":\"profflag\"})\n",
    "display(enrichdf3.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba4555bb-f09e-4dd0-a263-c3f9e7147f92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Modify/replace (withColumn, select/selectExpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d96036e3-1f78-426b-ada0-4eda393fd7c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enrichdf4=enrichdf3.withColumn(\"profession\",col(\"sourcename\"))#This will replace the profession with sourcename\n",
    "display(enrichdf4.take(20))\n",
    "\n",
    "#or\n",
    "enrichdf4=enrichdf3.withColumn(\"profession\",concat(\"profession\",lit('-'),\"profflag\"))#This will modify/enrich the profession column with sourcename\n",
    "\n",
    "#or using select/selectExpr\n",
    "enrichdf4=enrichdf3.select(\"custid\",\"age\",\"firstname\",\"lastname\",concat(\"profession\",lit('-'),\"profflag\").alias(\"profession\"),\"sourcename\",\"datadt\",\"loaddt\",\"profflag\")\n",
    "\n",
    "#or use selectExpr\n",
    "enrichdf4=enrichdf3.selectExpr(\"custid\",\"age\",\"firstname\",\"lastname\",\"concat(profession,'-',profflag) as profession\",\"sourcename\",\"datadt\",\"loaddt\",\"profflag\")\n",
    "\n",
    "\n",
    "display(enrichdf4.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89eaff99-5513-4c1c-b7b9-064e6558a54e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Remove/Eliminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dd8c631-ffda-48bc-88b7-964a22b645a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#enrichdf4=enrichdf3.withColumn(\"profession\",col(\"sourcename\"))#Cannot be used\n",
    "#or using select/selectExpr (yes, but costly)\n",
    "enrichdf5=enrichdf4.select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"sourcename\",\"datadt\",\"loaddt\")\n",
    "#or use selectExpr (yes, but costly)\n",
    "enrichdf5=enrichdf4.selectExpr(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"sourcename\",\"datadt\",\"loaddt\")\n",
    "#or \n",
    "enrichdf5=enrichdf4.drop(\"profflag\")#right function to use from dropping\n",
    "display(enrichdf5.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "220d40a6-4e2f-4f95-aa3b-70e4ddf43892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Splitting & Melting/merging of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25fd14ce-fc13-47f1-a60a-fb0285593e5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Splitting of column\n",
    "splitdf=enrichdf5.withColumn(\"profflag\",split(\"profession\",'-'))\n",
    "splitdf=splitdf.withColumn(\"profession\",col(\"profflag\")[0])\n",
    "display(splitdf.take(20))\n",
    "\n",
    "\n",
    "splitdf=splitdf.withColumn(\"shortprof\",upper(substring(col(\"profession\"),1,3))).drop(\"profflag\")\n",
    "display(splitdf)\n",
    "\n",
    "#Merging of column\n",
    "mergeddf=splitdf.select(col(\"custid\"),\"age\",concat_ws(\" \",col(\"firstname\"),col(\"lastname\")).alias(\"fullname\"),\"profession\",\"sourcename\",\"datadt\",\"loaddt\",\"shortprof\")#usage of select will help us avoid chaining of withColumn,drop,select\n",
    "display(mergeddf.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c008f7-53a7-4813-8e21-b6510b5d4638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mergeddf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed3ffe7f-04a1-494b-9b68-a12f8b669fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Formatting and Typecasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36519ccd-217d-4495-877f-0145ca8a105d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "formateddf=mergeddf.withColumn(\"datadt\",to_date(col('datadt'),'yy/dd/MM'))\n",
    "formateddf.printSchema()\n",
    "display(formateddf.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48d138a1-0bb5-4dff-9fa7-a91c2147a675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Data Customization - Application of Tailored Business specific Rules\n",
    "a. User Defined Functions<br/>\n",
    "b. Building of Frameworks & Reusable Functions (We will learn very next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8da14de-c162-450a-a743-00bfdd136e4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upperadd(addingupper_case):\n",
    "   return addingupper_case.upper()\n",
    "print(upperadd(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f51164-ee63-4b15-80a5-fc6079b1863c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper,col\n",
    "formateddf1=formateddf.withColumn(\"fullname\",upper(col(\"fullname\")))\n",
    "display(formateddf1.limit(10))\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "udfupper=udf(upperadd)#promote normal python function to spark ready udf\n",
    "formateddf1=formateddf.withColumn(\"fullname\",udfupper(col(\"fullname\")))#if udf is inevitable, then we create despite of performance bottleneck\n",
    "formateddf1.explain()\n",
    "display(formateddf1.take(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "936abbea-6bf9-4356-8cab-56f4eabb3597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Create Python Custom Function with complex logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d414a109-82a4-4bf0-99bf-506b8031f870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Calculating age category from the given age of the customer\n",
    "def pythonAgeCat(dfcol):\n",
    "    dfcol=int(dfcol)\n",
    "    if dfcol is None:\n",
    "        return \"Unknown\"\n",
    "    elif dfcol<=10:\n",
    "        return \"child\"\n",
    "    elif dfcol>10 and dfcol<=18:\n",
    "        return \"teenager\"\n",
    "    elif dfcol>18 and dfcol<=30:\n",
    "        return \"young\"\n",
    "    elif dfcol>30 and dfcol<=50:\n",
    "        return \"middleaged\"\n",
    "    else:\n",
    "        return \"senior\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7673317c-b9ce-4d76-8a8f-4a06574ad64a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "sparkudfageCat=udf(pythonAgeCat)\n",
    "customdf=formateddf1.withColumn(\"agecat\",sparkudfageCat(\"age\"))\n",
    "display(customdf.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a357a4bc-5052-4efa-ae08-1c06ff68745c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####4. Data Curation/Processing (Pre Wrangling Stage) - Applying different levels of business logics, transformation, filtering, grouping, aggregation and limits applying different transformation functions<br/>\n",
    "Select, Filter<br/>\n",
    "Derive flags & Columns<br/>\n",
    "Format<br/>\n",
    "Group & Aggregate<br/>\n",
    "Limit<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf33fd5d-13cc-4371-a43b-11f39b501a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1.Select, Filter<br/>\n",
    "In terms of Performance Optimzation - I ensured to do Push Down Optimization by doing select(project) & Filter(predicate) of what ever the expected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee84e4f0-acff-47f7-961a-23017c41c82f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "selectdf=customdf.select(\"custid\",\"fullname\",col(\"profession\").alias(\"prof\"),\"sourcename\",\"age\",\"datadt\",\"loaddt\",\"shortprof\",\"agecat\")\n",
    "display(selectdf.take(10))\n",
    "\n",
    "selectdf=customdf.selectExpr(\"custid\",\"fullname\",\"profession as prof\",\"sourcename\",\"age\",\"datadt\",\"loaddt\",\"shortprof\",\"agecat\")\n",
    "display(selectdf.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d39daf-f618-4b26-82f0-62557fe344d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c151845-2d5e-44a3-b30f-0ffa9c4f8492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filterdf=selectdf.filter((col(\"age\")>40) & (col(\"age\")<50))\n",
    "display(filterdf.take(10))\n",
    "\n",
    "filterdf=selectdf.where(col)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB_ETL_workout",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
