{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e9db29-71e6-4ee6-a139-a2e6adafec63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Enterprise Fleet Analytics Pipeline: Focuses on the business outcome (analytics) and the domain (fleet/logistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60102b53-da15-4c74-a306-b675d15c78d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![logistics](logistics_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c798b032-39ac-4bf6-9992-78159146fb4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Download the data from the below gdrive and upload into the catalog\n",
    "https://drive.google.com/drive/folders/1J3AVJIPLP7CzT15yJIpSiWXshu1iLXKn?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ca9127-636a-4be2-ad66-6af734d83aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**1. Data Munging** -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73641445-2b65-4138-89f3-44e10449c278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Visibily/Manually opening the file and capture couple of data patterns (Manual Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73dfa929-db33-4455-b583-8077f0a202ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Create a catalog called logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48dbe5dd-18f6-427f-add7-aa49c3653213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists logistics;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144e4feb-fb6c-48a1-9bd9-8aebc0d6e9dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Created a schema, volume and directory as below manually\n",
    "/Volumes/logistics/logistics_schema/logistics_volume/logistics_dir/\n",
    "\n",
    "######Keep the below files in directory\n",
    "\n",
    "logistics_shipment_detail_3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e39c7afa-4e52-4abf-8142-3faa856e78a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Manual Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af20103-ee06-4c1d-b9c9-82e4392db798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "It is a Semi-Structured data with comma seperator (JSON)<br/>\n",
    "No Header, No comments, footer is there in the data<br/>\n",
    "File have list[dictioniries]<br/>\n",
    "Total columns are 11<br/>\n",
    "All columns has even data accross file.<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "920a3ca5-bbc3-47d8-9947-544ef51deb7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507bfd1d-8d3d-417b-8da8-074150a9eec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically try to find couple of data patterns applying below EDA (File: logistics_source1)\n",
    "1. Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "2. Analyse the schema, datatypes, columns etc.,\n",
    "3. Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5c8bccf-d8b1-40b9-bfc2-dd17070ed4fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f289696-7e81-4ddd-88fa-3b05bfe62b08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_source1=spark.read.csv(\"/Volumes/logistics/logistics_schema/logistics_volume/logistics_dir/logistics_source1\",header='True',inferSchema='True')\n",
    "#.toDF('shipment_id','first_name','last_name','age','role')\n",
    "print(\"printschema\")\n",
    "raw_source1.printSchema()\n",
    "print(\"summary\")\n",
    "display(raw_source1.summary())\n",
    "print(\"describe\")\n",
    "display(raw_source1.describe())\n",
    "print(\"count\")\n",
    "display(raw_source1.count())\n",
    "print(\"sample\")\n",
    "display(raw_source1.sample(.1))\n",
    "display(raw_source1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09130180-3a58-4041-b066-6fb2feb65821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA structure functions we can use\n",
    "raw_source1.printSchema()#I am realizing the id & age columns are having some non numerical values (supposed to be numeric)\n",
    "print(raw_source1.columns)#I am understanding the column numbers/order and the column names\n",
    "print(raw_source1.dtypes)#Realizing the datatype of every columns (even we can do programattic column & type identification for dynamic programming)\n",
    "for i in raw_source1.dtypes:\n",
    "    if i[1]=='string':\n",
    "        print(i[0])\n",
    "\n",
    "print(raw_source1.schema)#To identify the structure of the data in the StructType and StructField format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de27b31f-de9a-457b-be06-00ada4960419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Data Munging -  (File: logistics_source1  and logistics_source2)\n",
    "Without modifying the data, identify:<br>\n",
    "Shipment IDs that appear in both master_v1 and master_v2<br>\n",
    "Records where:<br>\n",
    "1. shipment_id is non-numeric\n",
    "2. age is not an integer<br>\n",
    "\n",
    "Count rows having:\n",
    "3. fewer columns than expected\n",
    "4. more columns than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f97700-cae1-4ab6-97e8-502ffbd1a6d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a Spark Session Object\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5fa395a-b706-4076-b633-4bfad039fb88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_source2=spark.read.csv(\"/Volumes/logistics/logistics_schema/logistics_volume/logistics_dir/logistics_source2\",header='true',inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ccb88b6-ae18-497a-9434-f58aaa8a72e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive EDA data functions we can use\n",
    "#We identified few patterns on this data\n",
    "#1. Deduplication of rows and given column(s)\n",
    "#2. Null values ratio across all columns\n",
    "#3. Distribution (Dense) of the data across all number columns\n",
    "#4. Min, Max values\n",
    "#5. StdDeviation - \n",
    "#6. Percentile - Distribution percentage from 0 to 100 in 4 quadrants of 25%\n",
    "print(\"actual count of the data source1\",raw_source1.count())\n",
    "print(\"actual count of the data source2\",raw_source2.count())\n",
    "print(\"de-duplicated record (all columns) count source1\",raw_source1.distinct().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count source2\",raw_source2.distinct().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count source1\",raw_source1.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count source2\",raw_source2.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given shipment_id column count source1\",raw_source1.dropDuplicates(['shipment_id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given shipment_id column count source2\",raw_source2.dropDuplicates(['shipment_id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"source1\")\n",
    "display(raw_source1.describe())\n",
    "display(raw_source1.summary())\n",
    "\n",
    "print(\"source2\")\n",
    "display(raw_source2.describe())\n",
    "display(raw_source2.summary())\n",
    "print(raw_source1.dtypes)\n",
    "print(raw_source2.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81fc736-6e9f-4093-9f40-8e047989b602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging** File: logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7b773a5-b1db-4b1f-bf3e-93067d0483ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Combining Data + Schema Merging (Structuring)\n",
    "1. Read both files without enforcing schema\n",
    "2. Align them into a single canonical schema: shipment_id,\n",
    "first_name,\n",
    "last_name,\n",
    "age,\n",
    "role,\n",
    "hub_location,\n",
    "vehicle_type,\n",
    "data_source\n",
    "3. Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffe73808-5fcb-43aa-928e-5ce63b65d1a8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768112214386}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "struct1=\"shipment_id string,first_name string,last_name string,age string,role string,data_source string\"\n",
    "active_mung_source1=spark.read.schema(struct1).csv(\"/Volumes/logistics/logistics_schema/logistics_volume/logistics_dir/logistics_source1\",header='true')\n",
    "active_mung_source1=active_mung_source1.withColumn('data_source',lit('system1'))\n",
    "struct2=\"shipment_id string,first_name string,last_name string,age string,role string,hub_location string,vehicle_type string,data_source string\"\n",
    "active_mung_source2=spark.read.schema(struct2).csv(\"/Volumes/logistics/logistics_schema/logistics_volume/logistics_dir/logistics_source2\",header='true')\n",
    "active_mung_source2=active_mung_source2.withColumn('data_source',lit('system2'))\n",
    "display(active_mung_source1)\n",
    "display(active_mung_source2)\n",
    "\n",
    "merged_df=active_mung_source1.unionByName(active_mung_source2,allowMissingColumns=True)\n",
    "display(merged_df)\n",
    "merged_df.write.csv(\"/Volumes/logistics/logistics_schema/logistics_volume/logistics_dir/logistics_merged\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f014cc5-7fe9-4b05-a4db-0c8273e444de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "merged_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628e4769-0e24-481b-8b5c-33204a91ed3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Cleansing, Scrubbing: \n",
    "Cleansing (removal of unwanted datasets)<br>\n",
    "1. Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role<br>\n",
    "2. Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name<br>\n",
    "3. Join Readiness Rule - Drop records where the join key is null: shipment_id<br>\n",
    "\n",
    "Scrubbing (convert raw to tidy)<br>\n",
    "4. Age Defaulting Rule - Fill NULL values in the age column with: -1<br>\n",
    "5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN<br>\n",
    "6. Invalid Age Replacement - Replace the following values in age:\n",
    "\"ten\" to -1\n",
    "\"\" to -1<br>\n",
    "7. Vehicle Type Normalization - Replace inconsistent vehicle types: \n",
    "truck to LMV\n",
    "bike to TwoWheeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aacc2043-89d7-454d-9172-40a741bce2b7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768129211780}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType,ShortType\n",
    "\n",
    "struct3 = StructType([\n",
    "    StructField('shipment_id', StringType(), True),\n",
    "    StructField('first_name', StringType(), True),\n",
    "    StructField('last_name', StringType(), True),\n",
    "    StructField('age', StringType(), True),\n",
    "    StructField('role', StringType(), True),\n",
    "    StructField('data_source', StringType(), False),\n",
    "    StructField('hub_location', StringType(), True),\n",
    "    StructField('vehicle_type', StringType(), True)\n",
    "])\n",
    "                   \n",
    "clean_raw_df=spark.read.schema(struct3).csv('/Volumes/logistics/logistics_schema/logistics_volume/logistics_dir/logistics_merged/', mode='permissive')\n",
    "clean_raw_df.printSchema()\n",
    "print(clean_raw_df.count())\n",
    "display(clean_raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883324dd-7c79-44c4-96bf-dc719948e2db",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768129229924}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768129237496}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      },
      "2": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768129244157}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 2
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"shipment_id or role is null, will get deleted\")\n",
    "\n",
    "cleanseddf=clean_raw_df.na.drop(how=\"any\",subset=[\"shipment_id\",\"role\"])\n",
    "display(cleanseddf)\n",
    "\n",
    "print(\"firstname and lastname is null, will get deleted\")\n",
    "cleanseddf=cleanseddf.na.drop(how=\"all\",subset=[\"first_name\",\"last_name\"])\n",
    "display(cleanseddf)\n",
    "\n",
    "print(\"Apply join readniess rule\")\n",
    "cleanseddf=cleanseddf.filter(cleanseddf[\"shipment_id\"].isNotNull())\n",
    "display(cleanseddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa15369-f093-4ec8-88a3-ec00673504a2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768129256979}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Scrubbing (convert raw to tidy)\n",
    "#4. Age Defaulting Rule - Fill NULL values in the age column with: -1\n",
    "#5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN\n",
    "#6. Invalid Age Replacement - Replace the following values in age: \"ten\" to -1 \"\" to -1\n",
    "7##. Vehicle Type Normalization - Replace inconsistent vehicle types: truck to LMV bike to TwoWheeler\n",
    "\n",
    "#scrubbeddf1=cleanseddf.na.fill('not provided',subset=[\"lastname\",\"profession\"])#fill will help us replace #nulls with some value\n",
    "#display(scrubbeddf1)\n",
    "#find_replace_values_dict1={'Pilot':'Captain','Actor':'Celeberity'}\n",
    "#find_replace_values_dict2={'not provided':'NA'}\n",
    "#scrubbeddf2=scrubbeddf1.na.replace(find_replace_values_dict1,subset=[\"profession\"])#fill function is #helping us find and replace the values\n",
    "#scrubbeddf3=scrubbeddf2.na.replace(find_replace_values_dict2,subset=[\"lastname\"])\n",
    "#display(scrubbeddf3)\n",
    "\n",
    "\n",
    "scrubbeddf1=cleanseddf.na.fill('-1',subset=[\"age\"])\n",
    "display(scrubbeddf1)\n",
    "\n",
    "scrubbeddf2=scrubbeddf1.na.fill('UNKNOWN',subset=[\"vehicle_type\"])\n",
    "display(scrubbeddf2)\n",
    "\n",
    "replace_dict1={'ten':'-1','\"\"':'-1'}\n",
    "scrubbeddf3=scrubbeddf2.na.replace(replace_dict1,subset=[\"age\"])\n",
    "display(scrubbeddf3)\n",
    "\n",
    "\n",
    "replace_dict2={'Truck':'LMV','Bike':'TwoWheeler'}\n",
    "scrubbeddf4=scrubbeddf3.na.replace(replace_dict2,subset=[\"vehicle_type\"])\n",
    "display(scrubbeddf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b089e58-4b74-41e5-b050-bbfa8d249467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460f19af-0de1-48b5-b52d-4452e9988afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(scrubbeddf3.where(\"id in ('4000001')\"))#before row level dedup\n",
    "#dedupdf1=scrubbeddf3.distinct()#It will remove the row level duplicates\n",
    "#display(dedupdf1.where(\"id in ('4000001')\"))\n",
    "\n",
    "#print(\"non prioritized deduplication, just remove the duplicates retaining only the first row\")\n",
    "#display(dedupdf1.coalesce(1).where(\"id in ('4000003')\"))#before col level dedup\n",
    "#dedupdf2=dedupdf1.coalesce(1).dropDuplicates(subset=[\"id\"])#It will remove the column level duplicates #(retaining the first row in the dataframe)\n",
    "#display(dedupdf2.where(\"id in ('4000003')\"))\n",
    "#print(\"prioritized deduplication based on age\")\n",
    "#display(dedupdf1.coalesce(1).where(\"id in ('4000003')\"))#before col level dedup\n",
    "#dedupdf1.coalesce(1).where(\"id in ('4000003')\").orderBy([\"id\",\"age\"],ascending=[True,False]).show(3)\n",
    "#dedupdf2=dedupdf1.coalesce(1).orderBy([\"id\",\"age\"],ascending=[True,False]).dropDuplicates(subset=[\"id\"])#It will remove the column level duplicates (retaining the first row in the dataframe)\n",
    "#display(dedupdf2.where(\"id in ('4000003')\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b9df173-7822-4081-bd78-ac8db15bc196",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768204816015}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "5": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768204988254}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 5
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(scrubbeddf4.where(\"shipment_id in ('5000002')\"))\n",
    "deduppdf1=scrubbeddf4.distinct()\n",
    "display(deduppdf1.where(\"shipment_id in ('5000002')\"))\n",
    "\n",
    "display(deduppdf1.coalesce(1).where(\"shipment_id in ('5000002')\"))#before col level dedup\n",
    "deduppdf2=deduppdf1.dropDuplicates(subset=[\"shipment_id\"])#It will remove the column level duplicates (retaining the first row in the dataframe\n",
    "display(deduppdf2.coalesce(1).where(\"shipment_id in ('5000002')\"))\n",
    "\n",
    "deduppdf3=deduppdf2.coalesce(1).orderBy([\"shipment_id\",\"age\"],ascending=[True,False]).dropDuplicates(subset=[\"shipment_id\"])#It will remove the column level duplicates (retaining the first row in the dataframe)\n",
    "display(deduppdf3.where(\"shipment_id in ('5000002')\"))\n",
    "display(deduppdf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e8f929e-8b9c-4510-8316-12adfb17a734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccc60af7-9398-4de0-93ce-1f5bf9dd5c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Detail Dataframe creation <br>\n",
    "1. Read Data from logistics_shipment_detail.json\n",
    "2. As this data is a clean json data, it doesn't require any cleansing or scrubbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08b1ea10-d425-46c3-83e5-396681a0b1bc",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768135607179}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType,FloatType,DateType,StringType,DateType,StructType,StructField\n",
    "structjson=StructType([StructField(\"shipment_id\",IntegerType(),True),\n",
    "                        StructField(\"order_id\",StringType(),True),\n",
    "                        StructField(\"source_city\",StringType(),True),\n",
    "                        StructField(\"destination_city\",StringType(),True),\n",
    "                        StructField(\"shipment_status\",StringType(),True),\n",
    "                        StructField(\"cargo_type\",StringType(),True),\n",
    "                        StructField(\"vehicle_type\",StringType(),True),\n",
    "                        StructField(\"payment_mode\",StringType(),True),\n",
    "                        StructField(\"shipment_weight_kg\",FloatType(),True),\n",
    "                        StructField(\"shipment_cost\",FloatType(),True),\n",
    "                        StructField(\"shipment_date\",StringType(),True)\n",
    "                    ])\n",
    "\n",
    "read_json=spark.read.schema(structjson).option(\"multiline\",True).json(\"dbfs:/Volumes/logistics/logistics_schema/logistics_volume/logistics_dir/logistics_shipment_detail_3000.json\")\n",
    "display(read_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cd9b438-99be-431d-a81a-493c23b2b998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Standardizations:<br>\n",
    "\n",
    "1. Add a column<br> \n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>: domain as 'Logistics',  current timestamp 'ingestion_timestamp' and 'False' as 'is_expedited'\n",
    "2. Column Uniformity: \n",
    "role - Convert to lowercase<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "vehicle_type - Convert values to UPPERCASE<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json\n",
    "hub_location - Convert values to initcap case<br>\n",
    "Source Files: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "3. Format Standardization:<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json<br>\n",
    "Convert shipment_date to yyyy-MM-dd<br>\n",
    "Ensure shipment_cost has 2 decimal precision<br>\n",
    "4. Data Type Standardization<br>\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2) <br>\n",
    "age: Cast String to Integer<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "shipment_weight_kg: Cast to Double<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "is_expedited: Cast to Boolean<br>\n",
    "5. Naming Standardization <br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "Rename: first_name to staff_first_name<br>\n",
    "Rename: last_name to staff_last_name<br>\n",
    "Rename: hub_location to origin_hub_city<br>\n",
    "6. Reordering columns logically in a better standard format:<br>\n",
    "Source File: DF of Data from all 3 files<br>\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12d7c134-54cd-4a1a-b89f-0de7e4783897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "stand_addcol_json=read_json.withColumn(\"domain\",lit(\"Logistics\")).withColumn(\"ingestion_timestamp\",lit(datetime.now())).withColumn(\"is_expedited\",lit(False))\n",
    "display(stand_addcol_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3bbcd72-5b20-4607-ac0c-3f1b2b3d9db9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768202907714}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,lit,lower,upper,initcap\n",
    "stand_case1=deduppdf3.withColumn(\"role\",lower(col(\"role\")))\n",
    "display(stand_case1)\n",
    "\n",
    "stand_case_json=stand_addcol_json.withColumn(\"vehicle_type\",upper(col(\"vehicle_type\")))\n",
    "display(stand_case_json)\n",
    "\n",
    "stand_case2=stand_case1.withColumn(\"hub_location\",initcap(col(\"hub_location\")))\n",
    "display(stand_case2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ff2f2f0-0f0e-4341-ac0a-abd01a8cc5bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "stand_case2.where(\"shipment_id rlike '[a-zA-Z]'\").show()#rlike is regular expression like function that help us identify any string data in our DF column\n",
    "stand_case2.where(\"age rlike '[^0-9]'\").show()#checking for any non number values in age column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca8d8118-bfb7-402b-afb7-23a525455431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace,replace\n",
    "#Let's apply scrubbing features to our id column to replace ten with 10 (or we can think of using GenAI here)\n",
    "replaceval={'one':'1','two':'2','three':'3','four':'4','five':'5','six':'6','seven':'7','eight':'8','nine':'9','ten':'10'}\n",
    "stand_replace1=stand_case2.na.replace(replaceval,[\"shipment_id\"])\n",
    "display(stand_replace1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7466f687-2148-49b5-95a5-66f3aae6a137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Format Standardization:<br>\n",
    "#Source Files: DF of logistics_shipment_detail_3000.json<br>\n",
    "#Convert shipment_date to yyyy-MM-dd<br>\n",
    "#Ensure shipment_cost has 2 decimal precision<br>\n",
    "from pyspark.sql.functions import to_date\n",
    "stand_date_json=stand_case_json.withColumn(\"shipment_date\",to_date(col(\"shipment_date\"),'yy-MM-dd'))\n",
    "display(stand_date_json)\n",
    "stand_cast_json1=stand_date_json.withColumn(\"shipment_cost\",col(\"shipment_cost\").cast('decimal(20,2)'))\n",
    "display(stand_cast_json1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "500f2065-a6c9-471a-942c-b421407d80e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4.Data Type Standardization\n",
    "#Standardizing column data types to fix schema drift and enable mathematical operations.\n",
    "#Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "#age: Cast String to Integer\n",
    "#Source File: DF of logistics_shipment_detail_3000.json\n",
    "#shipment_weight_kg: Cast to Double\n",
    "#Source File: DF of logistics_shipment_detail_3000.json\n",
    "#is_expedited: Cast to Boolean\n",
    "\n",
    "stand_cast1=stand_replace1.withColumn(\"shipment_id\",col(\"shipment_id\").cast(\"integer\")).withColumn(\"age\",col(\"age\").cast(\"int\"))\n",
    "stand_cast1.printSchema()\n",
    "display(stand_cast1)\n",
    "\n",
    "stand_cast_json2=stand_cast_json1.withColumn(\"shipment_weight_kg\",col(\"shipment_weight_kg\").cast(\"double\")).withColumn(\"is_expedited\",col(\"is_expedited\").cast(\"boolean\"))\n",
    "stand_cast_json2.printSchema()\n",
    "display(stand_cast_json2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2562801-f9c9-4def-8150-3ea2cd601625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5.Naming Standardization\n",
    "#Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "#Rename: first_name to staff_first_name\n",
    "#Rename: last_name to staff_last_name\n",
    "#Rename: hub_location to origin_hub_city\n",
    "\n",
    "stand_name1=stand_cast1.withColumnsRenamed({\"first_name\":\"staff_first_name\",\"last_name\":\"staff_last_name\",\"hub_location\":\"origin_hub_city\"})\n",
    "display(stand_name1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b202831-948e-4062-a06a-df1595cac492",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768375664995}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6. Reordering columns logically in a better standard format:<br>\n",
    "#Source File: DF of Data from all 3 files<br>\n",
    "#shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)\n",
    "\n",
    "#union_df=stand_name1.unionByName(stand_cast_json2,allowMissingColumns=True)\n",
    "#union_df1=union_df.select(\"shipment_id\",\"staff_first_name\",\"staff_last_name\",\"role\",\"origin_hub_city\",\"age\",\"shipment_cost\",\"ingestion_timestamp\")\n",
    "#display(union_df1)\n",
    "\n",
    "stand_reorder=stand_name1.select(\"shipment_id\",\"staff_first_name\",\"staff_last_name\",\"role\",\"origin_hub_city\",\"age\",\"vehicle_type\")   \n",
    "display(stand_reorder)\n",
    "stand_reorder_json=stand_cast_json2.select(\"shipment_id\",\"order_id\",\"source_city\",\"destination_city\",\"shipment_status\",\"cargo_type\",\"vehicle_type\",\"payment_mode\",\"shipment_weight_kg\",\"shipment_cost\",\"shipment_date\",\"domain\"\"ingestion_timestamp\",\"is_expedited\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bafa5c8-7355-4d4f-9caf-40e0e0d303f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deduplication:\n",
    "1. Apply Record Level De-Duplication\n",
    "2. Apply Column Level De-Duplication (Primary Key Enforcement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f77a761b-34f2-4df6-82df-caf0172a8d08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dedup_row=union_df1.distinct()\n",
    "#display(dedup_row)\n",
    "#dedup_col=dedup_row.dropDuplicates([\"shipment_id\"])\n",
    "#display(dedup_col)\n",
    "dedup_row=stand_name1.distinct()\n",
    "munged_csv=dedup_row.dropDuplicates(subset=[\"shipment_id\"])\n",
    "display(munged_csv)\n",
    "dedup_row_json=stand_cast_json2.distinct()\n",
    "munged_json=dedup_row_json.dropDuplicates(subset=[\"shipment_id\"])\n",
    "display(munged_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "184ecd12-8081-4c8d-925e-d47c84e520d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Data Enrichment - Detailing of data\n",
    "Makes your data rich and detailed <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311d404f-b7d4-4e56-9f09-9367ec05e283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Adding of Columns (Data Enrichment)\n",
    "*Creating new derived attributes to enhance traceability and analytical capability.*\n",
    "\n",
    "**1. Add Audit Timestamp (`load_dt`)**\n",
    "Source File: logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    "* **Action:** Add a column `load_dt` using the function `current_timestamp()`.\n",
    "\n",
    "**2. Create Full Name (`full_name`)**\n",
    "Source File: logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The reporting dashboard requires a single field for the driver's name instead of separate columns.\n",
    "* **Action:** Create `full_name` by concatenating `first_name` and `last_name` with a space separator.\n",
    "* **Result:** \"Rajesh\" + \" \" + \"Kumar\" -> **\"Rajesh Kumar\"**\n",
    "\n",
    "**3. Define Route Segment (`route_segment`)**\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    "* **Action:** Combine `source_city` and `destination_city` with a hyphen.\n",
    "* **Result:** \"Chennai\" + \"-\" + \"Pune\" -> **\"Chennai-Pune\"**\n",
    "\n",
    "**4. Generate Vehicle Identifier (`vehicle_identifier`)**\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** We need a unique tracking code that immediately tells us the vehicle type and the shipment ID.\n",
    "* **Action:** Combine `vehicle_type` and `shipment_id` to create a composite key.\n",
    "* **Result:** \"Truck\" + \"_\" + \"500001\" -> **\"Truck_500001\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97f73d16-73bb-484d-bcdc-78aa98152f25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Add Audit Timestamp (load_dt) Source File: logistics_source1 and logistics_source2\n",
    "\n",
    "#Scenario: We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    "#Action: Add a column load_dt using the function current_timestamp().\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "enrich_csv1=munged_csv.withColumn(\"load_dt\",current_timestamp())\n",
    "display(enrich_csv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1f14c7b-ba7c-4179-87b2-3b8585776534",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768387755851}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Create Full Name (full_name) Source File: logistics_source1 and logistics_source2\n",
    "\n",
    "#Scenario: The reporting dashboard requires a single field for the driver's name instead of separate columns.\n",
    "#Action: Create full_name by concatenating first_name and last_name with a space separator.\n",
    "#Result: \"Rajesh\" + \" \" + \"Kumar\" -> \"Rajesh Kumar\"\n",
    "from pyspark.sql.functions import concat_ws\n",
    "enrich_csv2=enrich_csv1.select(\"shipment_id\",\"staff_first_name\",\"staff_last_name\",concat_ws(\" \",col(\"staff_first_name\"),col(\"staff_last_name\")).alias(\"full_name\"),\"age\",\"role\",\"data_source\",\"origin_hub_city\",\"vehicle_type\",\"load_dt\")\n",
    "display(enrich_csv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "607ea1dd-671c-43be-a9a7-ac7b294d432c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768387030616}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Define Route Segment (route_segment) Source File: logistics_shipment_detail_3000.json\n",
    "\n",
    "#Scenario: The logistics team wants to analyze performance based on specific transport lanes (Source to #Destination).\n",
    "#Action: Combine source_city and destination_city with a hyphen.\n",
    "#Result: \"Chennai\" + \"-\" + \"Pune\" -> \"Chennai-Pune\"\n",
    "\n",
    "from pyspark.sql.functions import concat_ws\n",
    "munged_json.printSchema()\n",
    "enrich_json1=munged_json.select(\"shipment_id\",\"order_id\",\"source_city\",\"destination_city\",concat_ws(\"-\",col(\"source_city\"),col(\"destination_city\")).alias(\"route_segment\"),\"shipment_status\",\"cargo_type\",\"vehicle_type\",\"payment_mode\",\"shipment_weight_kg\",\"shipment_cost\",\"shipment_date\",\"domain\",\"ingestion_timestamp\",\"is_expedited\")\n",
    "display(enrich_json1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40a6147-330d-4168-898a-2bca03186e04",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768395203847}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4. Generate Vehicle Identifier (vehicle_identifier) Source File: logistics_shipment_detail_3000.json\n",
    "\n",
    "#Scenario: We need a unique tracking code that immediately tells us the vehicle type and the shipment ID.\n",
    "#Action: Combine vehicle_type and shipment_id to create a composite key.\n",
    "#Result: \"Truck\" + \"_\" + \"500001\" -> \"Truck_500001\"\n",
    "\n",
    "enrich_csv3=enrich_csv2.select(\"*\",concat_ws(\"_\",col(\"vehicle_type\"),col(\"shipment_id\")).alias(\"vehicle_identifier\"))\n",
    "display(enrich_csv3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e525645-18d1-4a16-9909-afc89a2ed57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Deriving of Columns (Time Intelligence)\n",
    "*Extracting temporal features from dates to enable period-based analysis and reporting.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "**1. Derive Shipment Year (`shipment_year`)**\n",
    "* **Scenario:** Management needs an annual performance report to compare growth year-over-year.\n",
    "* **Action:** Extract the year component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **2024**\n",
    "\n",
    "**2. Derive Shipment Month (`shipment_month`)**\n",
    "* **Scenario:** Analysts want to identify seasonal peaks (e.g., increased volume in December).\n",
    "* **Action:** Extract the month component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **4** (April)\n",
    "\n",
    "**3. Flag Weekend Operations (`is_weekend`)**\n",
    "* **Scenario:** The Operations team needs to track shipments handled during weekends to calculate overtime pay or analyze non-business day capacity.\n",
    "* **Action:** Flag as **'True'** if the `shipment_date` falls on a Saturday or Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58371c82-8806-4b59-8ba1-1c8a73765e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extracting temporal features from dates to enable period-based analysis and reporting.\n",
    "#Source File: logistics_shipment_detail_3000.json\n",
    "#1. Derive Shipment Year (shipment_year)\n",
    "\n",
    "#Scenario: Management needs an annual performance report to compare growth year-over-year.\n",
    "#Action: Extract the year component from shipment_date.\n",
    "#Result: \"2024-04-23\" -> 2024\n",
    "\n",
    "enrich_json2=enrich_json1.selectExpr(\"shipment_id\",\"order_id\",\"source_city\",\"destination_city\",\"route_segment\",\"shipment_status\",\"cargo_type\",\"vehicle_type\",\"payment_mode\",\"shipment_weight_kg\",\"shipment_cost\",\"shipment_date\",\"domain\",\"ingestion_timestamp\",\"is_expedited\",\"substr(shipment_date,1,4) as shipement_year\")\n",
    "display(enrich_json2)\n",
    "#enrich_json2=enrich_json1.selectExpr(\"shipment_id\",\"source_city\",\"destination_city\",\"route_segment\",\"shipment_weight_kg\",\"is_expedited\",\"shipment_cost\",\"ingestion_timestamp\",\n",
    "#enrich_json2=enrich_json1.selectExpr(\"*\",substr(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b35eea2f-b545-4c75-a8e5-4159b65d1105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Derive Shipment Month (shipment_month)\n",
    "\n",
    "#Scenario: Analysts want to identify seasonal peaks (e.g., increased volume in December).\n",
    "#Action: Extract the month component from shipment_date.\n",
    "#Result: \"2024-04-23\" -> 4 (April)\n",
    "from pyspark.sql.functions import substring\n",
    "enrich_json3=enrich_json2.select(\"shipment_id\",\"order_id\",\"source_city\",\"destination_city\",\"route_segment\",\"shipment_status\",\"cargo_type\",\"vehicle_type\",\"payment_mode\",\"shipment_weight_kg\",\"shipment_cost\",\"shipment_date\",\"domain\",\"ingestion_timestamp\",\"is_expedited\",\"shipement_year\",substring(\"shipment_date\",6,2).alias(\"shipment_month\"))\n",
    "display(enrich_json3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f3cb8a2-1771-45e7-984f-b9a48cf0b845",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768392883856}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Flag Weekend Operations (is_weekend)\n",
    "\n",
    "#Scenario: The Operations team needs to track shipments handled during weekends to calculate overtime pay or #analyze non-business day capacity.\n",
    "#Action: Flag as 'True' if the shipment_date falls on a Saturday or Sunday.\n",
    "\n",
    "enrich_json3=enrich_json2.selectExpr(\"*\",\"\"\"case when dayofweek(shipment_date) in (2,5) then False\n",
    "                                     else True end as is_weekend\"\"\")\n",
    "display(enrich_json3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f178441f-3675-448e-b8f1-f45336851f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Enrichment/Business Logics (Calculated Fields)\n",
    "*Deriving new metrics and financial indicators using mathematical and date-based operations.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "\n",
    "**1. Calculate Unit Cost (`cost_per_kg`)**\n",
    "* **Scenario:** The Finance team wants to analyze the efficiency of shipments by determining the cost incurred per unit of weight.\n",
    "* **Action:** Divide `shipment_cost` by `shipment_weight_kg`.\n",
    "* **Logic:** `shipment_cost / shipment_weight_kg`\n",
    "\n",
    "**2. Track Shipment Age (`days_since_shipment`)**\n",
    "* **Scenario:** The Operations team needs to monitor how long it has been since a shipment was dispatched to identify potential delays.\n",
    "* **Action:** Calculate the difference in days between the `current_date` and the `shipment_date`.\n",
    "* **Logic:** `datediff(current_date(), shipment_date)`\n",
    "\n",
    "**3. Compute Tax Liability (`tax_amount`)**\n",
    "* **Scenario:** For invoicing and compliance, we must calculate the Goods and Services Tax (GST) applicable to each shipment.\n",
    "* **Action:** Calculate 18% GST on the total `shipment_cost`.\n",
    "* **Logic:** `shipment_cost * 0.18`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d59b6c-c2af-4066-ac38-0c85fe6370d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Deriving new metrics and financial indicators using mathematical and date-based operations.\n",
    "#Source File: logistics_shipment_detail_3000.json\n",
    "\n",
    "#1. Calculate Unit Cost (cost_per_kg)\n",
    "\n",
    "#Scenario: The Finance team wants to analyze the efficiency of shipments by determining the cost incurred #per unit of weight.\n",
    "#Action: Divide shipment_cost by shipment_weight_kg.\n",
    "#Logic: shipment_cost / shipment_weight_kg\n",
    "\n",
    "enrich_json4=enrich_json3.selectExpr(\"*\",\"try_divide(shipment_cost,shipment_weight_kg) as cost_per_kg\")\n",
    "enrich_json4=enrich_json4.withColumn(\"cost_per_kg\",col(\"cost_per_kg\").cast('decimal(20,2)'))\n",
    "display(enrich_json4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11558239-275f-4b45-9916-eb83d94923b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Track Shipment Age (days_since_shipment)\n",
    "\n",
    "#Scenario: The Operations team needs to monitor how long it has been since a shipment was dispatched to #identify potential delays.\n",
    "#Action: Calculate the difference in days between the current_date and the shipment_date.\n",
    "#Logic: datediff(current_date(), shipment_date)\n",
    "\n",
    "enrich_json5=enrich_json4.selectExpr(\"*\",\"\"\"datediff(current_date(),shipment_date) as days_since_shipment\"\"\")\n",
    "display(enrich_json5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec1ff20d-ac39-4678-a0ea-8059884fe063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Compute Tax Liability (tax_amount)\n",
    "\n",
    "#Scenario: For invoicing and compliance, we must calculate the Goods and Services Tax (GST) applicable to each shipment.\n",
    "#Action: Calculate 18% GST on the total shipment_cost.\n",
    "#Logic: shipment_cost * 0.18\n",
    "\n",
    "enrich_json6=enrich_json5.selectExpr(\"*\",\"\"\"shipment_cost*0.18 as tax_amount\"\"\")\n",
    "display(enrich_json6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de85d4f-d903-46fd-b110-1476a2383d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Remove/Eliminate (drop, select, selectExpr)\n",
    "*Excluding unnecessary or redundant columns to optimize storage and privacy.*<br>\n",
    "Source File: logistics_source1 and logistics_source2<br>\n",
    "\n",
    "**1. Remove Redundant Name Columns**\n",
    "* **Scenario:** Since we have already created the `full_name` column in the Enrichment step, the individual name columns are now redundant and clutter the dataset.\n",
    "* **Action:** Drop the `first_name` and `last_name` columns.\n",
    "* **Logic:** `df.drop(\"first_name\", \"last_name\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "831af6bc-d9f3-4a44-b717-b8449276c4a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Remove/Eliminate (drop, select, selectExpr)\n",
    "#Excluding unnecessary or redundant columns to optimize storage and privacy.\n",
    "#Source File: logistics_source1 and logistics_source2\n",
    "\n",
    "#1. Remove Redundant Name Columns\n",
    "\n",
    "#Scenario: Since we have already created the full_name column in the Enrichment step, the individual name columns are now redundant and clutter the dataset.\n",
    "#Action: Drop the first_name and last_name columns.\n",
    "#Logic: df.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "enrich_csv4=enrich_csv3.drop(\"staff_first_name\", \"staff_last_name\")\n",
    "display(enrich_csv4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7682d4f7-a188-4f86-b60f-c1afa5db220f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Splitting & Merging/Melting of Columns\n",
    "*Reshaping columns to extract hidden values or combine fields for better analysis.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "**1. Splitting (Extraction)**\n",
    "*Breaking one column into multiple to isolate key information.*\n",
    "* **Split Order Code:**\n",
    "  * **Action:** Split `order_id` (\"ORD100000\") into two new columns:\n",
    "    * `order_prefix` (\"ORD\")\n",
    "    * `order_sequence` (\"100000\")\n",
    "* **Split Date:**\n",
    "  * **Action:** Split `shipment_date` into three separate columns for partitioning:\n",
    "    * `ship_year` (2024)\n",
    "    * `ship_month` (4)\n",
    "    * `ship_day` (23)\n",
    "\n",
    "**2. Merging (Concatenation)**\n",
    "*Combining multiple columns into a single unique identifier or description.*\n",
    "* **Create Route ID:**\n",
    "  * **Action:** Merge `source_city` (\"Chennai\") and `destination_city` (\"Pune\") to create a descriptive route key:\n",
    "    * `route_lane` (\"Chennai->Pune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "884421e9-bff1-46be-945a-274bd12f08b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Split Order Code:\n",
    "#Action: Split order_id (\"ORD100000\") into two new columns:\n",
    "#order_prefix (\"ORD\")\n",
    "#order_sequence (\"100000\")\n",
    "\n",
    "enrich_json7=enrich_json6.select(\"*\",\n",
    "    regexp_replace(col(\"order_id\"), \"[^a-zA-Z0-9]\", \"\").alias(\"order_id_cleaned\"))\n",
    "enrich_json8=enrich_json7.withColumn(\"order_prefix\",col(\"order_id_cleaned\").substr(0,3)).withColumn(\"order_sequence\",col(\"order_id_cleaned\").substr(4,12))\n",
    "enrich_json8=enrich_json8.drop(\"order_id_cleaned\")\n",
    "#enrich_json8=enrich_json8.withColumn(\"temp_order\",split(\"order_id\",'_'))\n",
    "#enrich_json8=enrich_json8.withColumn(\"temp_order1\",col(\"temp_order\").getItem(0))\n",
    "display(enrich_json8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de2bd785-9b56-420d-ae57-bbd18a6dc312",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "#Split Date:\n",
    "#Action: Split shipment_date into three separate columns for partitioning:\n",
    "#ship_year (2024)\n",
    "#ship_month (4)\n",
    "#ship_day (23)\n",
    "from pyspark.sql.functions import split,column\n",
    "enrich_json9=enrich_json8.withColumn(\"ship_year\",split(\"shipment_date\",\"-\").getItem(0)).withColumn(\"ship_month\",split(\"shipment_date\",\"-\").getItem(1)).withColumn(\"ship_day\",split(\"shipment_date\",\"-\").getItem(2))\n",
    "\n",
    "#enrich_json9=enrich_json8.withColumn(\"splitcolumn\",col(\"ship_\").split('-'))\n",
    "display(enrich_json9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a214a2-9569-4fe3-98fc-7fc109c00966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Merging (Concatenation) Combining multiple columns into a single unique identifier or description.\n",
    "\n",
    "#Create Route ID:\n",
    "#Action: Merge source_city (\"Chennai\") and destination_city (\"Pune\") to create a descriptive route key:\n",
    "#route_lane (\"Chennai->Pune\")\n",
    "\n",
    "enrich_json10=enrich_json9.selectExpr(\"*\",\"concat(source_city,'-->',destination_city) as route_lane\")\n",
    "display(enrich_json10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a4891e-0c24-40b4-8ed8-b124efed02f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Data Customization & Processing - Application of Tailored Business Specific Rules\n",
    "\n",
    "### **UDF1: Complex Incentive Calculation**\n",
    "**Scenario:** The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "\n",
    "**Action:** Create a Python function `calculate_bonus(role, age)` and register it as a Spark UDF.\n",
    "\n",
    "**Logic:**\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` > 50:\n",
    "  * `Bonus` = 15% of Salary (Reward for Seniority)\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` < 30:\n",
    "  * `Bonus` = 5% of Salary (Encouragement for Juniors)\n",
    "* **ELSE**:\n",
    "  * `Bonus` = 0\n",
    "\n",
    "**Result:** A new derived column `projected_bonus` is generated for every row in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **UDF2: PII Masking (Privacy Compliance)**\n",
    "**Scenario:** For the analytics dashboard, we must hide the full identity of the staff to comply with privacy laws (GDPR/DPDP), while keeping names recognizable for internal managers.\n",
    "\n",
    "**Business Rule:** Show the first 2 letters, mask the middle characters with `****`, and show the last letter.\n",
    "\n",
    "**Action:** Create a UDF `mask_identity(name)`.\n",
    "\n",
    "**Example:**\n",
    "* **Input:** `\"Rajesh\"`\n",
    "* **Output:** `\"Ra****h\"`\n",
    "<br>\n",
    "**Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94826f3a-eadb-4a0a-8b6e-f24083d3838f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UDF1: Complex Incentive Calculation"
    }
   },
   "outputs": [],
   "source": [
    "#UDF1: Complex Incentive Calculation\n",
    "#Scenario: The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "#Action: Create a Python function calculate_bonus(role, age, salary) and register it as a Spark UDF.\n",
    "#Logic:\n",
    "#IF Role == 'Driver' AND Age > 50:\n",
    "#Bonus = 15% of Salary (Reward for Seniority)\n",
    "#IF Role == 'Driver' AND Age < 30:\n",
    "#Bonus = 5% of Salary (Encouragement for Juniors)\n",
    "#ELSE:\n",
    "#Bonus = 0\n",
    "#Result: A new derived column projected_bonus is generated for every row in the dataset.\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions\n",
    "\n",
    "def calculate_bonus(role, age, salary):\n",
    "    if role == 'driver' and age > 50:\n",
    "        return salary * 0.15\n",
    "    elif role == 'driver' and age < 30:\n",
    "        return salary * 0.05\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "spark.udf.register(\"employee_bonus\", calculate_bonus)\n",
    "salary=10000\n",
    "datacustom_csv1 = enrich_csv4.withColumn(\"projected_bonus\", functions.expr(f\"employee_bonus(role, age, {salary})\"))\n",
    "display(datacustom_csv1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3cf0390-2e29-48ec-82e0-acec2c24c9c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#UDF2: PII Masking (Privacy Compliance)\n",
    "#Scenario: For the analytics dashboard, we must hide the full identity of the staff to comply with privacy laws (GDPR/DPDP), while keeping names recognizable for internal managers.\n",
    "#Business Rule: Show the first 2 letters, mask the middle characters with ****, and show the last letter.\n",
    "#Action: Create a UDF mask_identity(name).\n",
    "#Example:\n",
    "#Input: \"Rajesh\"\n",
    "#Output: \"Ra****h\"\n",
    "#**Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions\n",
    "\n",
    "def mask_identity(name):\n",
    "    mask_name=name[0]+name[1]+'****'+name[-1]\n",
    "    return mask_name\n",
    "\n",
    "\n",
    "spark.udf.register(\"name_masking\",mask_identity)\n",
    "#enrich_json11=enrich_json10.selectExpr(\"*\",\"employee_bonus(role,age) as projected_bonus\")\n",
    "salary=10000\n",
    "datacustom_csv2=datacustom_csv1.withColumn(\"masked_name\",functions.expr(\"name_masking(full_name)\"))\n",
    "display(datacustom_csv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ed3d160-f54e-4bf3-bb2c-40ae7ff1b7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Data Core Curation & Processing (Pre-Wrangling)\n",
    "*Applying business logic to focus, filter, and summarize data before final analysis.*\n",
    "\n",
    "**1. Select (Projection)**<br>\n",
    "Source Files: logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The Driver App team only needs location data, not sensitive HR info.\n",
    "* **Action:** Select only `first_name`, `role`, and `hub_location`.\n",
    "\n",
    "**2. Filter (Selection)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** We need a report on active operational problems.\n",
    "* **Action:** Filter rows where `shipment_status` is **'DELAYED'** or **'RETURNED'**.\n",
    "* **Scenario:** Insurance audit for senior staff.\n",
    "* **Action:** Filter rows where `age > 50`.\n",
    "\n",
    "**3. Derive Flags & Columns (Business Logic)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** Identify high-value shipments for security tracking.\n",
    "* **Action:** Create flag `is_high_value` = **True** if `shipment_cost > 50,000`.\n",
    "* **Scenario:** Flag weekend operations for overtime calculation.\n",
    "* **Action:** Create flag `is_weekend` = **True** if day is Saturday or Sunday.\n",
    "\n",
    "**4. Format (Standardization)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** Finance requires readable currency formats.\n",
    "* **Action:** Format `shipment_cost` to string like **\"30,695.80\"**.\n",
    "* **Scenario:** Standardize city names for reporting.\n",
    "* **Action:** Format `source_city` to Uppercase (e.g., \"chennai\"  **\"CHENNAI\"**).\n",
    "\n",
    "**5. Group & Aggregate (Summarization)**<br>\n",
    "Source Files: logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** Regional staffing analysis.\n",
    "* **Action:** Group by `hub_location` and **Count** the number of staff.\n",
    "* **Scenario:** Fleet capacity analysis.\n",
    "* **Action:** Group by `vehicle_type` and **Sum** the `shipment_weight_kg`.\n",
    "\n",
    "**6. Sorting (Ordering)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** Prioritize the most expensive shipments.\n",
    "* **Action:** Sort by `shipment_cost` in **Descending** order.\n",
    "* **Scenario:** Organize daily dispatch schedule.\n",
    "* **Action:** Sort by `shipment_date` (Ascending) then `priority_flag` (Descending).\n",
    "\n",
    "**7. Limit (Top-N Analysis)**<br>\n",
    "Source File: json<br>\n",
    "* **Scenario:** Dashboard snapshot of critical delays.\n",
    "* **Action:** Filter for 'DELAYED', Sort by Cost, and **Limit to top 10** rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de08d9e-a24d-4251-84e5-aa63ec2d27a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4. Data Core Curation & Processing (Pre-Wrangling)\n",
    "#Applying business logic to focus, filter, and summarize data before final analysis.\n",
    "#1. Select (Projection)\n",
    "#Source Files: logistics_source1 and logistics_source2\n",
    "#Scenario: The Driver App team only needs location data, not sensitive HR info.\n",
    "#Action: Select only first_name, role, and hub_location.\n",
    "\n",
    "datacuration_csv1=datacustom_csv2.select(\"full_name\",\"role\",\"origin_hub_city\")\n",
    "display(datacuration_csv1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae38f2b1-7c75-41c6-8d92-0be87449e3aa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Lookup: Validate hub_location against Master_City_List"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f53e36-4648-4e55-b20a-29a4a646393d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Filter (Selection)\n",
    "#Source File: json\n",
    "#Scenario: We need a report on active operational problems.\n",
    "#Action: Filter rows where shipment_status is 'DELAYED' or 'RETURNED'.\n",
    "#Scenario: Insurance audit for senior staff.\n",
    "#Action: Filter rows where age > 50.\n",
    "\n",
    "datacuration_json1=enrich_json10.filter((col(\"shipment_status\")==\"DELAYED\") | (col(\"shipment_status\")==\"RETURNED\"))\n",
    "display(datacuration_json1)\n",
    "datacuration_csv1=datacustom_csv2.where(col(\"age\")>50)\n",
    "display(datacuration_csv1)\n",
    "\n",
    "#3. Group By (Aggregation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2142fbe5-953a-41c9-9d78-7634e8952530",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768498000721}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Derive Flags & Columns (Business Logic)\n",
    "#Source File: json\n",
    "\n",
    "#Scenario: Identify high-value shipments for security tracking.\n",
    "#Action: Create flag is_high_value = True if shipment_cost > 50,000.\n",
    "#Scenario: Flag weekend operations for overtime calculation.\n",
    "#Action: Create flag is_weekend = True if day is Saturday or Sunday.\n",
    "\n",
    "datacuration_json2=enrich_json10.selectExpr(\"*\",\"\"\"case when shipment_cost >50000 then 'True' else 'False' end as is_high_value\"\"\")\n",
    "display(datacuration_json2)\n",
    "datacuration_json3=enrich_json10.selectExpr(\"*\",\"\"\"case when dayofweek(shipment_date) not in (1,5) then false else true end as is_wekend\"\"\")\n",
    "display(datacuration_json3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "875c660c-57a6-4b34-9175-331967250d91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4. Format (Standardization)\n",
    "#Source File: json\n",
    "#Scenario: Finance requires readable currency formats.\n",
    "#Action: Format shipment_cost to string like \"30,695.80\".\n",
    "#Scenario: Standardize city names for reporting.\n",
    "#Action: Format source_city to Uppercase (e.g., \"chennai\"  \"CHENNAI\").\n",
    "enrich_json10.printSchema()\n",
    "datacuration_json4=datacuration_json3.withColumn(\"shipment_cost\",col(\"shipment_cost\").cast(\"string\")).withColumn((\"shipment_cost\"),concat_ws(\"\",lit(\"\\u20B9\"),col(\"shipment_cost\")))\n",
    "display(datacuration_json4)\n",
    "#datacuration_json5=enrich_json10.withColumn(\"source_city\",upper(col(\"source_city\")))\n",
    "datacuration_json5=datacuration_json4.withColumn(\"source_city\",upper(col(\"source_city\")))\n",
    "display(datacuration_json5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07694d70-867a-4693-b50b-a96fc17510b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Group & Aggregate (Summarization)"
    }
   },
   "outputs": [],
   "source": [
    "#5. Group & Aggregate (Summarization)\n",
    "#Source Files: logistics_source1 and logistics_source2\n",
    "#Scenario: Regional staffing analysis.\n",
    "#Action: Group by hub_location and Count the number of staff.\n",
    "#Scenario: Fleet capacity analysis.\n",
    "#Action: Group by vehicle_type and Sum the shipment_weight_kg.\n",
    "from pyspark.sql.functions import count, sum, col\n",
    "\n",
    "datacustom_csv3 = datacustom_csv2.groupBy(\"origin_hub_city\").agg(count(\"full_name\").alias(\"count\"))\n",
    "display(datacustom_csv3)\n",
    "datacuration_json6 = datacuration_json5.groupBy(\"vehicle_type\").agg(sum(col(\"shipment_weight_kg\").cast(\"double\")).alias(\"sum\"))\n",
    "display(datacuration_json6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff6ef24f-f766-4dc6-83fb-d583536fe15d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6. Sorting (Ordering)\n",
    "#Source File: json\n",
    "#Scenario: Prioritize the most expensive shipments.\n",
    "#Action: Sort by shipment_cost in Descending order.\n",
    "#Scenario: Organize daily dispatch schedule.\n",
    "#Action: Sort by shipment_date (Ascending) then priority_flag (Descending).\n",
    "\n",
    "datacuration_json7=datacuration_json5.orderBy(\"shipment_cost\",ascending=False)\n",
    "display(datacuration_json7)\n",
    "datacuration_json8=datacuration_json5.orderBy(col(\"shipment_date\"), col(\"is_weekend\"), ascending=[True, False])\n",
    "display(datacuration_json8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "300ee54b-f497-42a3-8e28-241ff9777ad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#7. Limit (Top-N Analysis)\n",
    "#Source File: json\n",
    "#Scenario: Dashboard snapshot of critical delays.\n",
    "#Action: Filter for 'DELAYED', Sort by Cost, and Limit to top 10 rows.\n",
    "\n",
    "datacuration_json9=enrich_json10.where(col(\"shipment_status\")==\"DELAYED\").orderBy(\"shipment_cost\").limit(10)\n",
    "display(datacuration_json9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "224c81b4-a922-4461-b7f1-2804f87c6bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datacuration_json9.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9992924-2d11-4cfa-b8fa-5c96bf6d0475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Data Wrangling - Transformation & Analytics\n",
    "*Combining, modeling, and analyzing data to answer complex business questions.*\n",
    "\n",
    "### **1. Joins**\n",
    "Source Files:<br>\n",
    "Left Side (staff_df):<br> logistics_source1 & logistics_source2<br>\n",
    "Right Side (shipments_df):<br> logistics_shipment_detail_3000.json<br>\n",
    "#### **1.1 Frequently Used Simple Joins (Inner, Left)**\n",
    "* **Inner Join (Performance Analysis):**\n",
    "  * **Scenario:** We only want to analyze *completed work*. Connect Staff to the Shipments they handled.\n",
    "  * **Action:** Join `staff_df` and `shipments_df` on `shipment_id`.\n",
    "  * **Result:** Returns only rows where a staff member is assigned to a valid shipment.\n",
    "* **Left Join (Idle Resource check):**\n",
    "  * **Scenario:** Find out which staff members are currently *idle* (not assigned to any shipment).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right) on `shipment_id`. Filter where `shipments_df.shipment_id` is NULL.\n",
    "\n",
    "#### **1.2 Infrequent Simple Joins (Self, Right, Full, Cartesian)**\n",
    "* **Self Join (Peer Finding):**\n",
    "  * **Scenario:** Find all pairs of employees working in the same `hub_location`.\n",
    "  * **Action:** Join `staff_df` to itself on `hub_location`, filtering where `staff_id_A != staff_id_B`.\n",
    "* **Right Join (Orphan Data Check):**\n",
    "  * **Scenario:** Identify shipments in the system that have *no valid driver* assigned (Data Integrity Issue).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right). Focus on NULLs on the left side.\n",
    "* **Full Outer Join (Reconciliation):**\n",
    "  * **Scenario:** A complete audit to find *both* idle drivers AND unassigned shipments in one view.\n",
    "  * **Action:** Perform a Full Outer Join on `shipment_id`.\n",
    "* **Cartesian/Cross Join (Capacity Planning):**\n",
    "  * **Scenario:** Generate a schedule of *every possible* driver assignment to *every* pending shipment to run an optimization algorithm.\n",
    "  * **Action:** Cross Join `drivers_df` and `pending_shipments_df`.\n",
    "\n",
    "#### **1.3 Advanced Joins (Semi and Anti)**\n",
    "* **Left Semi Join (Existence Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *at least one* shipment.\" (Standard filtering).\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_semi\")`.\n",
    "  * **Benefit:** Performance optimization; it stops scanning the right table once a match is found.\n",
    "* **Left Anti Join (Negation Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *never* touched a shipment.\"\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_anti\")`.\n",
    "\n",
    "### **2. Lookup**<br>\n",
    "Source File: logistics_source1 and logistics_source2 (merged into Staff DF)<br>\n",
    "* **Scenario:** Validation. Check if the `hub_location` in the staff file exists in the corporate `Master_City_List`.\n",
    "* **Action:** Compare values against a reference list.\n",
    "\n",
    "### **3. Lookup & Enrichment**<br>\n",
    "Source File: logistics_source1 and logistics_source2 (merged into Staff DF)<br>\n",
    "* **Scenario:** Geo-Tagging.\n",
    "* **Action:** Lookup `hub_location` (\"Pune\") in a Master Latitude/Longitude table and enrich the dataset by adding `lat` and `long` columns for map plotting.\n",
    "\n",
    "### **4. Schema Modeling (Denormalization)**<br>\n",
    "Source Files: All 3 Files (logistics_source1, logistics_source2, logistics_shipment_detail_3000.json)<br>\n",
    "* **Scenario:** Creating a \"Gold Layer\" Table for PowerBI/Tableau.\n",
    "* **Action:** Flatten the Star Schema. Join `Staff`, `Shipments`, and `Vehicle_Master` into one wide table (`wide_shipment_history`) so analysts don't have to perform joins during reporting.\n",
    "\n",
    "### **5. Windowing (Ranking & Trends)**<br>\n",
    "Source Files:<br>\n",
    "logistics_source2: Provides hub_location (Partition Key).<br>\n",
    "logistics_shipment_detail_3000.json: Provides shipment_cost (Ordering Key)<br>\n",
    "* **Scenario:** \"Who are the Top 3 Drivers by Cost in *each* Hub?\"\n",
    "* **Action:**\n",
    "  1. Partition by `hub_location`.\n",
    "  2. Order by `total_shipment_cost` Descending.\n",
    "  3. Apply `dense_rank()` and `row_number()\n",
    "  4. Filter where `rank or row_number <= 3`.\n",
    "\n",
    "### **6. Analytical Functions (Lead/Lag)**<br>\n",
    "Source File: <br>\n",
    "logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** Idle Time Analysis.\n",
    "* **Action:** For each driver, calculate the days elapsed since their *previous* shipment.\n",
    "\n",
    "### **7. Set Operations**<br>\n",
    "Source Files: logistics_source1 and logistics_source2<br>\n",
    "* **Union:** Combining `Source1` (Legacy) and `Source2` (Modern) into one dataset (Already done in Active Munging).\n",
    "* **Intersect:** Identifying Staff IDs that appear in *both* Source 1 and Source 2 (Duplicate/Migration Check).\n",
    "* **Except (Difference):** Identifying Staff IDs present in Source 2 but *missing* from Source 1 (New Hires).\n",
    "\n",
    "### **8. Grouping & Aggregations (Advanced)**<br>\n",
    "Source Files:<br>\n",
    "logistics_source2: Provides hub_location and vehicle_type (Grouping Dimensions).<br>\n",
    "logistics_shipment_detail_3000.json: Provides shipment_cost (Aggregation Metric).<br>\n",
    "* **Scenario:** The CFO wants a subtotal report at multiple levels:\n",
    "  1. Total Cost by Hub.\n",
    "  2. Total Cost by Hub AND Vehicle Type.\n",
    "  3. Grand Total.\n",
    "* **Action:** Use `cube(\"hub_location\", \"vehicle_type\")` or `rollup()` to generate all these subtotals in a single query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31653784-aa55-4f1f-b8a9-55ce21167414",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5. Data Wrangling - Transformation & Analytics\n",
    "#Combining, modeling, and analyzing data to answer complex business questions.\n",
    "\n",
    "#1.1 Frequently Used Simple Joins (Inner, Left)\n",
    "#Inner Join (Performance Analysis):\n",
    "#Scenario: We only want to analyze completed work. Connect Staff to the Shipments they handled.\n",
    "#Action: Join staff_df and shipments_df on shipment_id.\n",
    "#Result: Returns only rows where a staff member is assigned to a valid shipment.\n",
    "#Left Join (Idle Resource check):\n",
    "#Scenario: Find out which staff members are currently idle (not assigned to any shipment).\n",
    "#Action: Join staff_df (Left) with shipments_df (Right) on shipment_id. Filter where shipments_df.#shipment_id is NULL.##\n",
    "\n",
    "staff_df=datacustom_csv2\n",
    "shipments_df=datacuration_json8\n",
    "inner_join_df=staff_df.join(shipments_df,how='inner',on='shipment_id')\n",
    "display(inner_join_df)\n",
    "\n",
    "left_join_df=staff_df.join(shipments_df,how='left',on='shipment_id').where(shipments_df.shipment_id.isNull())\n",
    "display(left_join_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beef10b7-299e-429a-9b82-fcfefb640c09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.2 Infrequent Simple Joins (Self, Right, Full, Cartesian)\n",
    "#Self Join (Peer Finding):\n",
    "#Scenario: Find all pairs of employees working in the same hub_location.\n",
    "#Action: Join staff_df to itself on hub_location, filtering where staff_id_A != staff_id_B.\n",
    "#Right Join (Orphan Data Check):\n",
    "#Scenario: Identify shipments in the system that have no valid driver assigned (Data Integrity Issue).\n",
    "#Action: Join staff_df (Left) with shipments_df (Right). Focus on NULLs on the left side.\n",
    "#Full Outer Join (Reconciliation):\n",
    "#Scenario: A complete audit to find both idle drivers AND unassigned shipments in one view.\n",
    "#Action: Perform a Full Outer Join on shipment_id.\n",
    "#Cartesian/Cross Join (Capacity Planning):\n",
    "#Scenario: Generate a schedule of every possible driver assignment to every pending shipment to run an optimization algorithm.\n",
    "#Action: Cross Join drivers_df and pending_shipments_df.\n",
    "\n",
    "staff_df.printSchema()\n",
    "self_join_df=staff_df.alias(\"staff_id_A\").join(staff_df.alias(\"staff_id_B\"),how='inner',on='origin_hub_city').where(col(\"staff_id_A.shipment_id\") != col(\"staff_id_B.shipment_id\"))\n",
    "display(self_join_df)\n",
    "\n",
    "right_join_df=staff_df.join(shipments_df,how='right',on='shipment_id')\n",
    "display(right_join_df)\n",
    "\n",
    "full_outer_join_df=staff_df.join(shipments_df,how='full',on='shipment_id')\n",
    "display(full_outer_join_df)\n",
    "\n",
    "cartesian_join_df=staff_df.crossJoin(shipments_df)\n",
    "display(cartesian_join_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d383e8e-ce1f-43b8-b230-8e589c9d1292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.3 Advanced Joins (Semi and Anti)\n",
    "#Left Semi Join (Existence Check):\n",
    "#Scenario: \"Show me the details of Drivers who have at least one shipment.\" (Standard filtering).\n",
    "#Action: staff_df.join(shipments_df, \"shipment_id\", \"left_semi\").\n",
    "#Benefit: Performance optimization; it stops scanning the right table once a match is found.\n",
    "#Left Anti Join (Negation Check):\n",
    "#Scenario: \"Show me the details of Drivers who have never touched a shipment.\"\n",
    "#Action: staff_df.join(shipments_df, \"shipment_id\", \"left_anti\").\n",
    "\n",
    "left_semi_join_df=staff_df.join(shipments_df,how='leftsemi',on='shipment_id')\n",
    "display(left_semi_join_df)\n",
    "\n",
    "left_anti_join_df=staff_df.join(shipments_df,how='leftanti',on='shipment_id')\n",
    "display(left_anti_join_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c9f9902-ba90-4443-bb79-2c98f80e8a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "#2. Lookup\n",
    "#Source File: logistics_source1 and logistics_source2 (merged into Staff DF)\n",
    "\n",
    "#Scenario: Validation. Check if the hub_location in the staff file exists in the corporate Master_City_List.\n",
    "#Action: Compare values against a reference list.\n",
    "master_city_list_df=spark.read.csv(\"/Volumes/logistics/logistics_schema/logistics_volume/logistics_dir/Master_City_List.csv\",header='True',inferSchema='True')\n",
    "display(master_city_list_df)\n",
    "\n",
    "lookup_df=staff_df.join(master_city_list_df,staff_df[\"origin_hub_city\"]==master_city_list_df[\"city_name\"],how='leftsemi')\n",
    "display(lookup_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d12fceba-75d8-404a-9624-2920dec9e080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### **3. Lookup & Enrichment**<br>\n",
    "#Source File: logistics_source1 and logistics_source2 (merged into Staff DF)<br>\n",
    "#* **Scenario:** Geo-Tagging.\n",
    "#* **Action:** Lookup `hub_location` (\"Pune\") in a Master Latitude/Longitude table and enrich the dataset by adding `lat` and `long` columns for map plotting.\n",
    "\n",
    "lookup_enrich_df=staff_df.join(master_city_list_df,staff_df[\"origin_hub_city\"]==master_city_list_df[\"city_name\"],how='left').where(col(\"origin_hub_city\")== 'Pune')\n",
    "lookup_enrich_df=lookup_enrich_df.withColumnsRenamed({\"latitude\":\"lat\",\"longitude\":\"long\"})\n",
    "display(lookup_enrich_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aa2d193-84f3-4649-bb4e-832282306a0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4. Schema Modeling (Denormalization)\n",
    "#Source Files: All 3 Files (logistics_source1, logistics_source2, logistics_shipment_detail_3000.json)\n",
    "\n",
    "#Scenario: Creating a \"Gold Layer\" Table for PowerBI/Tableau.\n",
    "#Action: Flatten the Star Schema. Join Staff, Shipments, and Vehicle_Master into one wide table (wide_shipment_history) so analysts don't have to perform joins during reporting.\n",
    "\n",
    "wide_shipment_history_df=shipments_df.select('shipment_id','vehicle_type','shipment_cost','shipment_date')\n",
    "denormalized_df=staff_df.join(wide_shipment_history_df,how='inner',on='shipment_id')\n",
    "display(denormalized_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ede64520-b39e-464e-80c5-db6de87462bf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "#5. Windowing (Ranking & Trends)\n",
    "#Source Files:\n",
    "#logistics_source2: Provides hub_location (Partition Key).\n",
    "#logistics_shipment_detail_3000.json: Provides shipment_cost (Ordering Key)\n",
    "\n",
    "#Scenario: \"Who are the Top 3 Drivers by Cost in each Hub?\"\n",
    "#Action:\n",
    "#Partition by hub_location.\n",
    "#Order by total_shipment_cost Descending.\n",
    "#Apply dense_rank() and `row_number()\n",
    "#Filter where rank or row_number <= 3.\n",
    "#from pyspark.sql.functions import row_number,desc,dense_rank\n",
    "#from pyspark.sql.window import Window\n",
    "\n",
    "#windowing_df=denormalized_df.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"origin_hub_city\").orderBy(desc(\"shipment_cost\"))))\n",
    "#display(windowing_df)\n",
    "\n",
    "#dense_rank_df=denormalized_df.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"origin_hub_city\").orderBy(desc(col(\"shipment_cost\"))))\\.withColumn(\"dense_rnk\",row_number().over(Window.partitionBy(\"origin_hub_city\").orderBy(desc(\"shipment_cost\"))))\\.where(col(\"dense_rnk\") <= 3)\n",
    "#display(dense_rank_df)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import row_number, desc, dense_rank, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"origin_hub_city\").orderBy(desc(\"shipment_cost\"))\n",
    "\n",
    "windowing_df = denormalized_df.withColumn(\"seqnum\", row_number().over(window_spec))\n",
    "display(windowing_df)\n",
    "\n",
    "dense_rank_df = denormalized_df.withColumn(\"seqnum\", row_number().over(window_spec)) \\\n",
    "    .withColumn(\"dense_rnk\", dense_rank().over(window_spec)) \\\n",
    "    .where(col(\"dense_rnk\") <= 3)\n",
    "display(dense_rank_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32cd58a9-48c7-47c3-b32f-be047d8cf197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6. Analytical Functions (Lead/Lag)\n",
    "#Source File:\n",
    "#logistics_shipment_detail_3000.json\n",
    "\n",
    "#Scenario: Idle Time Analysis.\n",
    "#Action: For each driver, calculate the days elapsed since their previous shipment.\n",
    "\n",
    "#sk_orderjoinedf=orderjoineddf.withColumn(\"nexttransamt\",lead(\"amt\",1,-1).over(Window.partitionBy(\"custid\").orderBy(asc(\"txndt\")))).withColumn(\"priortransamt\",lag(\"amt\",1,-1).over(Window.partitionBy(\"custid\").orderBy(asc(\"txndt\"))))\n",
    "from pyspark.sql.functions import lead, lag,asc\n",
    "\n",
    "lead_lag_df=denormalized_df.withColumn(\"nexttransamt\",lead(\"shipment_cost\",1,-1).over(Window.partitionBy(\"full_name\").orderBy(asc(\"shipment_date\")))).withColumn(\"priortransamt\",lag(\"shipment_cost\",1,-1).over(Window.partitionBy(\"full_name\").orderBy(asc(\"shipment_date\"))))\n",
    "display(lead_lag_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe05098b-daac-4139-9163-d1995ff12d0b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Operations (Fixed NUM_COLUMNS_MISMATCH)"
    }
   },
   "outputs": [],
   "source": [
    "#7. Set Operations\n",
    "#Source Files: logistics_source1 and logistics_source2\n",
    "\n",
    "#Union: Combining Source1 (Legacy) and Source2 (Modern) into one dataset (Already done in Active Munging).\n",
    "#Intersect: Identifying Staff IDs that appear in both Source 1 and Source 2 (Duplicate/Migration Check).\n",
    "#Except (Difference): Identifying Staff IDs present in Source 2 but missing from Source 1 (New Hires).\n",
    "\n",
    "set_union_df=active_mung_source1.unionByName(active_mung_source2,allowMissingColumns=True)\n",
    "display(set_union_df)\n",
    "set_unionall_df=active_mung_source1.unionAll(active_mung_source1)\n",
    "display(set_unionall_df)\n",
    "\n",
    "# Minimal fix: select the same columns (in the same order) for intersect and subtract\n",
    "common_cols = [\"shipment_id\", \"first_name\", \"last_name\", \"age\", \"role\", \"data_source\"]\n",
    "set_intersect_df=active_mung_source1.select(common_cols).intersect(active_mung_source2.select(common_cols))\n",
    "display(set_intersect_df)\n",
    "set_subtract_df=active_mung_source2.select(common_cols).subtract(active_mung_source1.select(common_cols))\n",
    "display(set_subtract_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41606be5-28fd-4a71-86aa-4f549c78fd16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#8. Grouping & Aggregations (Advanced)\n",
    "#Source Files:\n",
    "#logistics_source2: Provides hub_location and vehicle_type (Grouping Dimensions).\n",
    "#logistics_shipment_detail_3000.json: Provides shipment_cost (Aggregation Metric).\n",
    "\n",
    "#Scenario: The CFO wants a subtotal report at multiple levels:\n",
    "#Total Cost by Hub.\n",
    "#Total Cost by Hub AND Vehicle Type.\n",
    "#Grand Total.\n",
    "#Action: Use cube(\"hub_location\", \"vehicle_type\") or rollup() to generate all these subtotals in a single query.\n",
    "group_agg_csv_df=enrich_csv4.groupBy(\"origin_hub_city\",\"vehicle_type\").count()\n",
    "display(group_agg_csv_df)\n",
    "\n",
    "enrich_json10.printSchema()\n",
    "group_agg_json_df=enrich_json10.groupBy(\"vehicle_type\").agg(sum(\"shipment_cost\"))\n",
    "display(group_agg_json_df)\n",
    "\n",
    "rollup_df=enrich_json10.rollup(\"vehicle_type\",\"shipment_cost\").agg(sum(\"shipment_cost\")).orderBy(\"vehicle_type\")\n",
    "display(rollup_df)\n",
    "\n",
    "cube_df=enrich_json10.cube(\"vehicle_type\",\"shipment_cost\").agg(sum(\"shipment_cost\")).orderBy(\"vehicle_type\",\"shipment_cost\")\n",
    "display(cube_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbdbf31-7a73-44f2-8e74-6c6fcf35d916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Data Persistance (LOAD)-> Data Publishing & Consumption<br>\n",
    "\n",
    "Store the inner joined, lookup and enrichment, Schema Modeling, windowing, analytical functions, set operations, grouping and aggregation data into the delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb2af694-5792-4103-86cf-6f5043dfca42",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write outputs (fixed invalid column name)"
    }
   },
   "outputs": [],
   "source": [
    "#mungeddf.write.csv(\"/Volumes/workspace/default/volumewe47_datalake/consumption/munged\",mode='overwrite')\n",
    "#formatteddf.write.json(\"/Volumes/workspace/default/volumewe47_datalake/consumption/enriched\",mode='overwrite')\n",
    "#sk_orderjoinedf.write.parquet(\"/Volumes/workspace/default/volumewe47_datalake/consumption/standardized\",mode='overwrite')\n",
    "#sk_orderjoinedf=orderjoineddf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"custid\").orderBy(desc(\"amt\"))))\n",
    "#sk_orderjoinedf.write.saveAsTable(\"cust_purchase_analytics\",mode='overwrite')\n",
    "#cubedf.write.saveAsTable(\"cust_purchase_analytics_cube\",mode='overwrite')\n",
    "#pivotdf.write.saveAsTable(\"cust_purchase_analytics_pivot\",mode='overwrite')\n",
    "\n",
    "munged_csv.write.csv(\"/Volumes/sparkio/etlschema/etlvolume/etl_dir/logistics/munged_csv\",mode='overwrite')\n",
    "munged_json.write.json(\"/Volumes/sparkio/etlschema/etlvolume/etl_dir/logistics/munged_json\",mode='overwrite')\n",
    "\n",
    "enrich_csv4.write.csv(\"/Volumes/sparkio/etlschema/etlvolume/etl_dir/logistics/enrich_csv\",mode='overwrite')\n",
    "enrich_json10.write.json(\"/Volumes/sparkio/etlschema/etlvolume/etl_dir/logistics/enriched_json\",mode='overwrite')\n",
    "\n",
    "datacuration_csv1.write.csv(\"/Volumes/sparkio/etlschema/etlvolume/etl_dir/logistics/curation_csv\",mode='overwrite')\n",
    "datacuration_json9.write.json(\"/Volumes/sparkio/etlschema/etlvolume/etl_dir/logistics/curation_json\",mode='overwrite')\n",
    "\n",
    "# Minimal fix: rename invalid column before saving as table\n",
    "cube_df_fixed = cube_df.withColumnRenamed('sum(shipment_cost)', 'sum_shipment_cost')\n",
    "cube_df_fixed.write.saveAsTable(\"shipment_cost_analystics_cube\",mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee7e8d0-1fbb-4a6c-af39-316374a94a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7.Take the copy of the above notebook and try to write the equivalent SQL for which ever applicable."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6240108784724777,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "5_BB2_Usecase2_DSL_SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
